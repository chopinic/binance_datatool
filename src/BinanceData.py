#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆåŠŸèƒ½ï¼š
1. è·å–æ‰€æœ‰äº¤æ˜“å¯¹
2. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½Kçº¿ã€ç”Ÿæˆå…¨æ¯ã€é—´éš™æ£€æµ‹ä¿®å¤å¹¶è¿”å›DataFrame
3. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½metricsæ•°æ®å¹¶è¿”å›DataFrame
"""

# æ·»åŠ å½“å‰é¡¹ç›®çš„srcç›®å½•åˆ°Pythonè·¯å¾„
import sys
from pathlib import Path
logLevel = "debug"  # "debug" æˆ– "run"ï¼Œdebugè¾“å‡ºæ‰€æœ‰æ—¥å¿—ï¼Œrunåªè¾“å‡ºå…³é”®æ­¥éª¤å’Œé”™è¯¯
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
project_root = Path(__file__).resolve().parent.parent
# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(project_root / "src"))

import asyncio
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import polars as pl
import polars.selectors as cs
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib import style

# ç½‘ç»œé…ç½®
GLOBAL_HTTP_PROXY = "http://127.0.0.1:7890"  # 7890ä»£ç†

# æ•°æ®è·¯å¾„é…ç½®
RootPath = r"D:\Codes\GitHub\BinanceData\Download"
DATA_DIR = Path(f"{RootPath}/raw_data")  # æ•°æ®ä¿å­˜ç›®å½•
PARSED_DATA_DIR = Path(f"{RootPath}/parsed_data")  # è§£æåçš„æ•°æ®ç›®å½•
OUTPUT_DIR = Path(f"{RootPath}/output")  # è¾“å‡ºç›®å½•

def setPath(rootPath):
    global RootPath
    global DATA_DIR
    global PARSED_DATA_DIR
    global OUTPUT_DIR
    RootPath = rootPath
    DATA_DIR = Path(f"{RootPath}/raw_data")  # æ•°æ®ä¿å­˜ç›®å½•
    PARSED_DATA_DIR = Path(f"{RootPath}/parsed_data")  # è§£æåçš„æ•°æ®ç›®å½•
    OUTPUT_DIR = Path(f"{RootPath}/output")  # è¾“å‡ºç›®å½•

    Path(RootPath).mkdir(exist_ok=True)
    DATA_DIR.mkdir(exist_ok=True)
    OUTPUT_DIR.mkdir(exist_ok=True)
    PARSED_DATA_DIR.mkdir(exist_ok=True)

# ä¸´æ—¶æ–‡ä»¶è·¯å¾„
WARNING_JSON = Path("./warning.json")  # è­¦å‘Šä¿¡æ¯æ–‡ä»¶

# æµ‹è¯•é…ç½®
TEST_SYMBOL = "BTCUSDT"
TEST_START_DATE = "2025-02-01"
TEST_END_DATE = "2025-02-10"

# è®¾ç½®matplotlibæ ·å¼
style.use('seaborn-v0_8-darkgrid')

# æ—¥å¿—å‡½æ•°
def printLog(message, level="info"):
    """
    æ—¥å¿—è¾“å‡ºå‡½æ•°ï¼Œæ ¹æ®logLevelæ§åˆ¶è¾“å‡º
    
    Args:
        message: æ—¥å¿—æ¶ˆæ¯
        level: æ—¥å¿—çº§åˆ«ï¼Œå¯é€‰å€¼ï¼š"run"ï¼ˆé‡è¦ä¿¡æ¯ï¼‰ã€"error"ï¼ˆé”™è¯¯ï¼‰ã€"debug"ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
    """
    if logLevel == "debug":
        # debugçº§åˆ«è¾“å‡ºæ‰€æœ‰æ—¥å¿—
        print(message)
    else:
        # runçº§åˆ«åªè¾“å‡ºå…³é”®æ­¥éª¤å¼€å§‹ç‚¹å’Œé”™è¯¯
        if level in ["run", "error"]:
            print(message)

from bdt_common.constants import HTTP_TIMEOUT_SEC
from bdt_common.enums import DataFrequency, DataType, TradeType
from bdt_common.network import create_aiohttp_session
from bdt_common.polars_utils import execute_polars_batch
from bhds.aws.client import create_aws_client_from_config
from bhds.aws.downloader import AwsDownloader
from bhds.aws.checksum import ChecksumVerifier
from bhds.aws.local import AwsDataFileManager
from bhds.aws.parser import create_aws_parser
from bhds.holo_kline.merger import Holo1mKlineMerger
from bhds.holo_kline.gap_detector import HoloKlineGapDetector
from bhds.holo_kline.splitter import HoloKlineSplitter
from bhds.holo_kline.resampler import HoloKlineResampler


async def get_all_um_symbols(http_proxy: str = "") -> List[str]:
    """
    è·å–æ‰€æœ‰UMäº¤æ˜“å¯¹
    
    Args:
        http_proxy: HTTPä»£ç†
    
    Returns:
        æ‰€æœ‰UMäº¤æ˜“å¯¹åˆ—è¡¨
    """
    if http_proxy == "":
        global GLOBAL_HTTP_PROXY
        http_proxy = GLOBAL_HTTP_PROXY
    async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
        client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.kline,
            data_freq=DataFrequency.daily,
            time_interval="1m",
            session=session,
            http_proxy=http_proxy
        )
        symbols = await client.list_symbols()
        return symbols


def filter_files_by_time_range(files: List[Path], start_date: str, end_date: str) -> List[Path]:
    """
    ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ä»¶
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        ç­›é€‰åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # åˆ†ç¦»zipæ–‡ä»¶å’ŒCHECKSUMæ–‡ä»¶
    zip_files = [f for f in files if f.name.endswith('.zip')]
    checksum_files = [f for f in files if f.name == 'CHECKSUM']
    
    filtered_files = []
    
    # å…ˆè¾“å‡ºæ‰€æœ‰è·å–åˆ°çš„zipæ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
    printLog(f"  å…±è·å–åˆ° {len(zip_files)} ä¸ªzipæ–‡ä»¶, {len(checksum_files)} ä¸ªCHECKSUMæ–‡ä»¶", level="debug")
    if zip_files:
        printLog(f"  æœ€æ–°çš„5ä¸ªæ–‡ä»¶: {', '.join([f.name for f in sorted(zip_files)[-5:]])}", level="debug")
    
    # æ—¥æœŸåŒ¹é…é€»è¾‘ - å¤„ç†zipæ–‡ä»¶
    for file_path in zip_files:
        # æ–‡ä»¶åæ ¼å¼ï¼šSYMBOL-TIME_INTERVAL-YYYY-MM-DD.zip
        # æˆ–ï¼šSYMBOL-YYYY-MM-DD.zip
        filename = file_path.name
        
        # æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸéƒ¨åˆ†
        import re
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', filename)
        if date_match:
            file_date = date_match.group()
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
            if start_date <= file_date <= end_date:
                filtered_files.append(file_path)
    
    # å°†CHECKSUMæ–‡ä»¶æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
    filtered_files.extend(checksum_files)
    
    return filtered_files


async def _download_single_symbol_data(
    http_proxy: str,
    symbol: str,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    ä¸‹è½½å•ä¸ªç¬¦å·çš„æŒ‡å®šç±»å‹æ•°æ®
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        data_type: æ•°æ®ç±»å‹ï¼ˆklineæˆ–metricsï¼‰
        time_interval: Kçº¿æ—¶é—´é—´éš”ï¼ˆmetricsä¸éœ€è¦ï¼‰
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ˜¯å¦æˆåŠŸä¸‹è½½
    """
    try:
        downloader = AwsDownloader(local_dir=DATA_DIR, http_proxy=http_proxy, verbose=True)
        verifier = ChecksumVerifier(delete_mismatch=False)
        
        async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = create_aws_client_from_config(
                trade_type=TradeType.um_futures,
                data_type=data_type,
                data_freq=DataFrequency.daily,
                time_interval=time_interval,
                session=session,
                http_proxy=http_proxy
            )
            
            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = await client.list_data_files(symbol)
            range_files = filter_files_by_time_range(files, start_date, end_date)
            
            if not range_files:
                printLog(f"æ²¡æœ‰æ‰¾åˆ° {symbol} çš„{data_type.value}æ–‡ä»¶")
                return False
            
            # ä¸‹è½½æ–‡ä»¶
            printLog(f"ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®...", level="run")
            printLog(f"  ä¸‹è½½æ–‡ä»¶åˆ—è¡¨ ({len(range_files)} ä¸ª):", level="debug")
            for file in range_files:
                printLog(f"    - {file.name}", level="debug")
            await downloader.aws_download(range_files)
            
            # éªŒè¯æ–‡ä»¶
            data_type_path = f"{DATA_DIR}/data/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
            if data_type == DataType.kline:
                data_type_path += f"/{time_interval}"
            
            symbol_dir = Path(data_type_path)
            manager = AwsDataFileManager(symbol_dir)
            unverified_files = manager.get_unverified_files()
            
            if unverified_files:
                results = verifier.verify_files(unverified_files)
                printLog(f"éªŒè¯å®Œæˆ: {results['success']} ä¸ªæˆåŠŸ, {results['failed']} ä¸ªå¤±è´¥")
                if results['failed'] > 0:
                    printLog(f"éªŒè¯å¤±è´¥è¯¦æƒ…: {results['errors']}", level="error")
                return results['failed'] == 0
            
            return True
    except Exception as e:
        import traceback
        printLog(f"ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®å¤±è´¥: {e}", level="error")
        printLog(traceback.format_exc())
        return False

async def _check_data_exists(
    symbol: str,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    æ£€æŸ¥æŒ‡å®šç±»å‹çš„æ•°æ®æ˜¯å¦å·²ç»ä¸‹è½½å¹¶éªŒè¯
    
    Args:
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        symbol: å¸å¯¹
        data_type: æ•°æ®ç±»å‹
        time_interval: Kçº¿æ—¶é—´é—´éš”
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ•°æ®æ˜¯å¦å­˜åœ¨ä¸”å·²éªŒè¯
    """
    from datetime import datetime, timedelta
    import re
    
    data_type_path = f"{DATA_DIR}/data/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
    if data_type == DataType.kline:
        data_type_path += f"/{time_interval}"
    
    symbol_dir = Path(data_type_path)
    if not symbol_dir.exists():
        return False
    
    manager = AwsDataFileManager(symbol_dir)
    verified_files = manager.get_verified_files()
    
    if not verified_files:
        return False
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    # æ”¶é›†æ‰€æœ‰å·²éªŒè¯æ–‡ä»¶çš„æ—¥æœŸ
    verified_dates = set()
    for file_path in verified_files:
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', file_path.name)
        if date_match:
            verified_dates.add(date_match.group())
    
    # æ‰¾å‡ºæ—¶é—´èŒƒå›´å†…ç¼ºå°‘çš„æ—¥æœŸ
    missing_dates = []
    current_dt = start_dt
    while current_dt <= end_dt:
        current_date_str = current_dt.strftime('%Y-%m-%d')
        if current_date_str not in verified_dates:
            missing_dates.append(current_date_str)
        current_dt += timedelta(days=1)
    
    # å¦‚æœæœ‰ç¼ºå°‘çš„æ—¥æœŸï¼Œæ‰“å°æ—¥å¿—å¹¶è¿”å›False
    if missing_dates:
        printLog(f"  ç¼ºå°‘ä»¥ä¸‹æ—¥æœŸçš„æ•°æ®: {', '.join(missing_dates[:5])}{'...' if len(missing_dates) > 5 else ''}")
        return False
    
    return True  # æ‰€æœ‰æ—¥æœŸçš„æ•°æ®éƒ½å­˜åœ¨

async def get_kline_dataframe(
    symbol: str,
    start_date: str,
    end_date: str,
    time_interval: str = "1m",
    frequency: str = "1h",
    http_proxy: str = "",
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        time_interval: Kçº¿æ—¶é—´é—´éš”
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        Kçº¿æ•°æ®çš„DataFrame
    """
    global PARSED_DATA_DIR
    global GLOBAL_HTTP_PROXY
    setPath(RootPath)

    printLog(f"\nè·å– {symbol} çš„Kçº¿æ•°æ®ï¼ˆ{start_date} ~ {end_date}ï¼‰...", level="run")

    if http_proxy == "":
        http_proxy = GLOBAL_HTTP_PROXY
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        symbol=symbol,
        data_type=DataType.kline,
        time_interval=time_interval,
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_type=DataType.kline,
            time_interval=time_interval,
            start_date=start_date,
            end_date=end_date
        )
    else:
        printLog(f"{symbol} çš„Kçº¿æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½",level="run")
    
    # è§£ææ•°æ®
    parse_downloaded_data(
        symbols=[symbol],
        time_interval=time_interval,
        start_date=start_date,
        end_date=end_date
    )
    
    # ç”Ÿæˆå…¨æ¯Kçº¿
    import tempfile
    with tempfile.TemporaryDirectory(prefix="um_holo_") as temp_dir:
        temp_path = Path(temp_dir)
        
        # ç”Ÿæˆå…¨æ¯Kçº¿
        holo_files = generate_holo_klines(PARSED_DATA_DIR, TradeType.um_futures, temp_path, symbols=[symbol], start_date=start_date, end_date=end_date)
        
        if not holo_files:
            printLog(f"æ— æ³•ç”Ÿæˆ {symbol} çš„å…¨æ¯Kçº¿", level="error")
            return pl.DataFrame()
        
        # æ£€æµ‹å’Œå¤„ç†é—´éš™
        # symbols_with_gaps, _ = detect_and_process_gaps(holo_files, start_date=start_date, end_date=end_date)
        
        # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
        resampled_dir = temp_path / "resampled"
        resampled_dir.mkdir(parents=True, exist_ok=True)
        resampled_files = resample_holo_klines(holo_files, resampled_dir, frequency)
        
        # è·å–æœ€ç»ˆDataFrame
        result_df = get_final_dataframe(resampled_files, symbol)
                
        return result_df

def plot_dataframe(
    df: pl.DataFrame,
    data_type: Optional[str] = None,
    symbol: Optional[str] = None,
    save_path: Optional[Path] = None,
    figsize: Tuple[int, int] = (12, 6)
) -> None:
    """
    ç»˜åˆ¶DataFrameæ•°æ®çš„æŠ˜çº¿å›¾
    
    Args:
        df: è¦ç»˜åˆ¶çš„DataFrame
        data_type: æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'kline'æˆ–'metrics'ï¼Œå¦‚æœä¸æä¾›å°†è‡ªåŠ¨æ£€æµ‹
        symbol: å¸å¯¹åç§°ï¼Œç”¨äºæ ‡é¢˜
        save_path: ä¿å­˜å›¾ç‰‡çš„è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™æ˜¾ç¤ºå›¾ç‰‡
        figsize: å›¾ç‰‡å°ºå¯¸
    """
    if df.is_empty():
        printLog("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾", level="error")
        return
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
    if data_type is None:
        if 'close' in df.columns:
            data_type = 'kline'
        else:
            data_type = 'metrics'
    
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=figsize)
    
    if data_type == 'kline':
        # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
        if 'candle_begin_time' in df.columns and 'close' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['candle_begin_time'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('candle_begin_time').str.to_datetime())
            
            # ç»˜åˆ¶æŠ˜çº¿å›¾
            plt.plot(df['candle_begin_time'], df['close'], label='Close Price', color='blue', linewidth=1.5)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.ylabel('Price')
            plt.title(f'{symbol} Close Price Chart' if symbol else 'Close Price Chart')
        else:
            printLog("Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'candle_begin_time' æˆ– 'close'", level="error")
            return
    
    elif data_type == 'metrics':
        # ç»˜åˆ¶metricsæ•°æ®
        if 'timestamp' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['timestamp'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('timestamp').str.to_datetime())
            
            # è·å–æ•°å€¼åˆ—
            numeric_columns = df.select([cs.numeric()]).columns
            
            # ç»˜åˆ¶æ‰€æœ‰æ•°å€¼åˆ—
            for col in numeric_columns:
                if col != 'timestamp':
                    plt.plot(df['timestamp'], df[col], label=col)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.title(f'{symbol} Metrics Chart' if symbol else 'Metrics Chart')
        else:
            printLog("Metricsæ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'timestamp'", level="error")
            return
    
    else:
        printLog(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}", level="error")
        return
    
    # æ·»åŠ ç½‘æ ¼å’Œå›¾ä¾‹
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾ç‰‡
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        printLog(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
    else:
        plt.show()
    
    # å…³é—­å›¾å½¢
    plt.close()


async def get_metrics_dataframe(
    symbol: str,
    start_date: str,
    end_date: str,
    http_proxy: str = "",
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
    
    Returns:
        Metricsæ•°æ®çš„DataFrame
    """
    global GLOBAL_HTTP_PROXY 
    if http_proxy == "":
        http_proxy = GLOBAL_HTTP_PROXY
    printLog(f"\nè·å– {symbol} çš„Metricsæ•°æ®ï¼ˆ{start_date} ~ {end_date}ï¼‰...",level="run")

    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        symbol=symbol,
        data_type=DataType.metrics,
        time_interval="",
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_type=DataType.metrics,
            time_interval="",
            start_date=start_date,
            end_date=end_date
        )
    else:
        printLog(f"{symbol} çš„Metricsæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    metrics_symbol_dir = f"{DATA_DIR}/data/{TradeType.um_futures.value}/daily/{DataType.metrics.value}/{symbol}"
    metrics_symbol_dir = Path(metrics_symbol_dir)
    if metrics_symbol_dir.exists():
        manager = AwsDataFileManager(metrics_symbol_dir)
        verified_files = manager.get_verified_files()
        
        if verified_files:
            try:
                # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ä»¶
                filtered_files = verified_files
                if start_date and end_date:
                    filtered_files = filter_files_by_time_range(verified_files, start_date, end_date)
                    
                    # åˆ†ç¦»zipæ–‡ä»¶å’Œå…¶ä»–æ–‡ä»¶
                    filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                    if filtered_zip_files:
                        printLog(f"     ç­›é€‰å‡º {len(filtered_zip_files)} ä¸ªMetricsæ–‡ä»¶åœ¨ {start_date} - {end_date} èŒƒå›´å†…", level="debug")
                    else:
                        printLog(f"æ²¡æœ‰æ‰¾åˆ°åœ¨ {start_date} - {end_date} èŒƒå›´å†…çš„Metricsæ–‡ä»¶")
                        return pl.DataFrame()
                else:
                    printLog(f"     æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè§£ææ‰€æœ‰ {len(verified_files)} ä¸ªMetricsæ–‡ä»¶", level="debug")
                
                # å°è¯•åˆ›å»ºmetricsè§£æå™¨
                metrics_parser = create_aws_parser(DataType.metrics)
                
                # åªå¤„ç†zipæ–‡ä»¶
                filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                if not filtered_zip_files:
                    printLog(f"æ²¡æœ‰æ‰¾åˆ°å¯è§£æçš„Metrics zipæ–‡ä»¶")
                    return pl.DataFrame()
                
                # è¯»å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¹¶åˆå¹¶
                dfs = []
                for zip_file in filtered_zip_files:
                    try:
                        # ä»zipæ–‡ä»¶è¯»å–CSVæ•°æ®
                        df = metrics_parser.read_csv_from_zip(zip_file)
                        dfs.append(df)
                        printLog(f"è§£æ {zip_file.name}", level="debug")
                    except Exception as e:
                        printLog(f"è§£æ {zip_file.name} å¤±è´¥: {e}", level="error")
                
                if dfs:
                    # åˆå¹¶æ‰€æœ‰DataFrame
                    combined_df = pl.concat(dfs)
                    printLog(f"æˆåŠŸè§£æ {symbol} çš„Metricsæ•°æ®ï¼Œå…± {len(combined_df)} è¡Œ",level="run")
                    return combined_df
                else:
                    printLog(f"æ²¡æœ‰æˆåŠŸè§£æä»»ä½•Metricsæ–‡ä»¶")
                    return pl.DataFrame()
                    
            except Exception as e:
                printLog(f"è§£æ {symbol} çš„Metricsæ•°æ®å¤±è´¥: {e}", level="error")
                printLog(f"å½“å‰ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒMetricsæ•°æ®çš„è§£æ")
                import traceback
                traceback.print_exc()
    
    return pl.DataFrame()


def parse_downloaded_data(
    symbols: List[str],
    time_interval: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> None:
    """
    è§£æä¸‹è½½çš„zipæ–‡ä»¶ä¸ºCSVæ ¼å¼
    
    Args:
        data_dir: ä¸‹è½½çš„æ•°æ®ç›®å½•
        symbols: å¸å¯¹åˆ—è¡¨
        time_interval: Kçº¿æ—¶é—´é—´éš”
        parsed_data_dir: è§£æåçš„æ•°æ®ä¿å­˜ç›®å½•
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰ï¼Œä»…è§£ææ­¤æ—¥æœŸä¹‹åçš„æ•°æ®
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰ï¼Œä»…è§£ææ­¤æ—¥æœŸä¹‹å‰çš„æ•°æ®
    """
    printLog(f"\nè§£æä¸‹è½½çš„æ•°æ®...")
    
    kline_parser = create_aws_parser(DataType.kline)
    
    for symbol in symbols:
        printLog(f"è§£æ {symbol}...", level="debug")
        
        # è§£æKçº¿æ•°æ®
        kline_symbol_dir = f"{DATA_DIR}/data/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
        kline_symbol_dir = Path(kline_symbol_dir)
        if kline_symbol_dir.exists():
            manager = AwsDataFileManager(kline_symbol_dir)
            verified_files = manager.get_verified_files()
            
            if verified_files:
                # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ä»¶
                filtered_files = verified_files
                if start_date and end_date:
                    filtered_files = filter_files_by_time_range(verified_files, start_date, end_date)
                    
                    # åˆ†ç¦»zipæ–‡ä»¶å’Œå…¶ä»–æ–‡ä»¶
                    filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                    if filtered_zip_files:
                        printLog(f"     ç­›é€‰å‡º {len(filtered_zip_files)} ä¸ªæ–‡ä»¶åœ¨ {start_date} - {end_date} èŒƒå›´å†…", level="debug")
                    else:
                        printLog(f"     æ²¡æœ‰æ‰¾åˆ°åœ¨ {start_date} - {end_date} èŒƒå›´å†…çš„æ–‡ä»¶", level="debug")
                        continue
                else:
                    printLog(f"     æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè§£ææ‰€æœ‰ {len(verified_files)} ä¸ªæ–‡ä»¶", level="debug")
                
                # ç¡®ä¿è§£æç›®å½•å­˜åœ¨ï¼ˆåŒ…å«data/å‰ç¼€ï¼‰
                symbol_parsed_dir = f"{PARSED_DATA_DIR}/data/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
                symbol_parsed_dir = Path(symbol_parsed_dir)
                symbol_parsed_dir.mkdir(parents=True, exist_ok=True)
                
                # æ¸…ç†æ—§çš„CSVæ–‡ä»¶
                for csv_file in symbol_parsed_dir.glob("*.csv"):
                    csv_file.unlink()
                    printLog(f"åˆ é™¤æ—§çš„CSVæ–‡ä»¶: {csv_file.name}", level="debug")
                
                # åªå¤„ç†zipæ–‡ä»¶
                for zip_file in [f for f in filtered_files if f.name.endswith('.zip')]:
                    try:
                        # ä»zipæ–‡ä»¶è¯»å–CSVæ•°æ®
                        df = kline_parser.read_csv_from_zip(zip_file)
                        
                        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
                        parquet_file = symbol_parsed_dir / f"{zip_file.stem}.parquet"
                        df.write_parquet(parquet_file)
                        printLog(f"è§£æ {zip_file.name} -> {parquet_file.name}", level="debug")
                    except Exception as e:
                        printLog(f"è§£æ {zip_file.name} å¤±è´¥: {e}", level="error")
    
    printLog("æ•°æ®è§£æå®Œæˆ")


def generate_holo_klines(
    parsed_data_dir: Path,
    trade_type: TradeType,
    output_dir: Path,
    symbols: Optional[List[str]] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> List[Path]:
    """
    ç”Ÿæˆå…¨æ¯kçº¿
    
    Args:
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        trade_type: äº¤æ˜“ç±»å‹
        output_dir: è¾“å‡ºç›®å½•
        symbols: è¦å¤„ç†çš„ç¬¦å·åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç¬¦å·
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        ç”Ÿæˆçš„å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
    """
    printLog(f"\nç”Ÿæˆå…¨æ¯kçº¿...")
    merger = Holo1mKlineMerger(
        trade_type=trade_type,
        base_dir=parsed_data_dir,
        include_vwap=True,
        include_funding=True,
    )
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºPolars datetimeå¯¹è±¡
    start_time = None
    end_time = None
    
    if start_date and end_date:
        from datetime import datetime
        import polars as pl
        
        # è§£ææ—¥æœŸå­—ç¬¦ä¸²
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # è½¬æ¢ä¸ºPolars datetimeå¯¹è±¡
        start_time = pl.datetime(
            year=start_dt.year, month=start_dt.month, day=start_dt.day,
            time_zone="UTC"
        )
        end_time = pl.datetime(
            year=end_dt.year, month=end_dt.month, day=end_dt.day,
            time_zone="UTC"
        ) + pl.duration(days=1) - pl.duration(microseconds=1)
    
    # ç”ŸæˆæŒ‡å®šç¬¦å·çš„å…¨æ¯kçº¿
    lazy_frames = merger.generate_all(output_dir, target_symbols=symbols, start_time=start_time, end_time=end_time)
    if not lazy_frames:
        printLog("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„ç¬¦å·", level="error")
        return []
    
    # æ‰§è¡ŒPolarsæ‰¹å¤„ç†ä»¥ç”Ÿæˆæ–‡ä»¶
    execute_polars_batch(lazy_frames, "Collecting kline data")
    
    # è·å–ç”Ÿæˆçš„æ–‡ä»¶
    generated_files = list(output_dir.glob("*.parquet"))
    printLog(f"ç”Ÿæˆ {len(generated_files)} ä¸ªå…¨æ¯kçº¿æ–‡ä»¶", level="debug")
    return generated_files


def detect_and_process_gaps(
    holo_files: List[Path],
    min_days: int = 1,
    min_price_chg: float = 0.1,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> Tuple[int, int]:
    """
    æ£€æµ‹å’Œå¤„ç†é—´éš™
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        min_days: æœ€å°é—´éš™å¤©æ•°
        min_price_chg: æœ€å°ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        (æœ‰é—´éš™çš„ç¬¦å·æ•°, ç”Ÿæˆçš„åˆ†å‰²æ–‡ä»¶æ•°)
    """
    printLog(f"\nğŸ”„ æ£€æµ‹é—´éš™...")
    printLog(f"     Min days: {min_days}", level="debug")
    printLog(f"     Min price change: {min_price_chg * 100}%", level="debug")
    
    detector = HoloKlineGapDetector(min_days, min_price_chg)
    splitter = HoloKlineSplitter(prefix="SP")
    
    # ç”Ÿæˆé—´éš™æ£€æµ‹ä»»åŠ¡
    gap_tasks = [detector.detect(file_path) for file_path in holo_files]
    gap_results = execute_polars_batch(gap_tasks, "Detecting gaps", return_results=True)
    
    symbols_with_gaps = 0
    total_splits = 0
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
    has_time_filter = False
    filter_start = None
    filter_end = None
    
    if start_date and end_date:
        from datetime import datetime
        # è§£ææ—¥æœŸå­—ç¬¦ä¸²
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # è½¬æ¢ä¸ºPolars datetimeå¯¹è±¡
        filter_start = pl.datetime(
            year=start_dt.year, month=start_dt.month, day=start_dt.day,
            time_zone="UTC"
        )
        filter_end = pl.datetime(
            year=end_dt.year, month=end_dt.month, day=end_dt.day,
            time_zone="UTC"
        ) + pl.duration(days=1) - pl.duration(microseconds=1)
        
        has_time_filter = True
    
    # å¤„ç†é—´éš™ç»“æœ
    for file_path, gaps_df in zip(holo_files, gap_results):
        if len(gaps_df) > 0:
            symbol = file_path.stem
            symbols_with_gaps += 1
            
            printLog(f"\nğŸ” {symbol} - {len(gaps_df)} gap(s)", level="debug")
            printLog("-" * 40, level="debug")
            
            # è¿‡æ»¤å‡ºæŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„é—´éš™
            if has_time_filter:
                gaps_df = gaps_df.filter(
                    (pl.col("prev_begin_time") >= filter_start) & 
                    (pl.col("candle_begin_time") <= filter_end)
                )
            
            for gap in gaps_df.sort("time_diff", descending=True).iter_rows(named=True):
                printLog(f"  {gap['prev_begin_time']} â†’ {gap['candle_begin_time']}", level="debug")
                printLog(f"  Duration: {gap['time_diff']}, Change: {gap['price_change']:.2%}", level="debug")
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„é—´éš™åˆ†å‰²kçº¿æ•°æ®
            printLog(f"  åˆ†å‰² {symbol}...", level="debug")
            split_files = splitter.split_file(file_path, gaps_df)
            total_splits += len(split_files)
            
            for split_file in split_files:
                seg_df = pl.read_parquet(split_file)
                min_begin_time = seg_df["candle_begin_time"].min()
                max_begin_time = seg_df["candle_begin_time"].max()
                printLog(f"    {split_file.name}: {len(seg_df)} è¡Œ, {min_begin_time} åˆ° {max_begin_time}", level="debug")

    printLog(f"\næ€»ç»“: {symbols_with_gaps}/{len(holo_files)} ä¸ªç¬¦å·æœ‰é—´éš™")
    printLog(f"         ç”Ÿæˆäº† {total_splits} ä¸ªåˆ†å‰²æ–‡ä»¶")
    
    return symbols_with_gaps, total_splits


def resample_holo_klines(holo_files: List[Path], output_dir: Path, frequency: str = "1h") -> List[Path]:
    """
    å°†å…¨æ¯kçº¿é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
    """
    printLog(f"\nå°†æ•°æ®é‡é‡‡æ ·åˆ°{frequency}...")
    
    # åˆå§‹åŒ–é‡é‡‡æ ·å™¨
    resampler = HoloKlineResampler(resample_interval=frequency)
    
    resampled_files = []
    for file_path in holo_files:
        symbol = file_path.stem
        output_file = output_dir / f"{symbol}_{frequency}.parquet"
        
        try:
            # è¯»å–å…¨æ¯kçº¿æ•°æ®
            df = pl.read_parquet(file_path)
            
            # å°†DataFrameè½¬æ¢ä¸ºLazyFrame
            ldf = df.lazy()
            
            # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
            resampled_ldf = resampler.resample(ldf)
            
            # è®¡ç®—å¹¶ä¿å­˜é‡é‡‡æ ·åçš„æ•°æ®
            resampled_df = resampled_ldf.collect()
            resampled_df.write_parquet(output_file)
            resampled_files.append(output_file)
            
            printLog(f"{symbol}: {len(df)} è¡Œ â†’ {len(resampled_df)} è¡Œ", level="debug")
        except Exception as e:
            printLog(f"{symbol}: é‡é‡‡æ ·å¤±è´¥ - {e}", level="error")
            import traceback
            traceback.print_exc()
    
    printLog(f"æˆåŠŸé‡é‡‡æ · {len(resampled_files)} ä¸ªæ–‡ä»¶åˆ°{frequency}")

    return resampled_files


def get_final_dataframe(resampled_files: List[Path], symbol: str) -> pl.DataFrame:
    """
    è·å–æœ€ç»ˆçš„DataFrame
    
    Args:
        resampled_files: é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
        symbol: è¦è·å–çš„ç¬¦å·
    
    Returns:
        æœ€ç»ˆçš„DataFrame
    """
    for file_path in resampled_files:
        if file_path.stem.startswith(symbol):
            return pl.read_parquet(file_path)
    
    printLog(f"æ²¡æœ‰æ‰¾åˆ° {symbol} çš„é‡é‡‡æ ·æ–‡ä»¶", level="error")
    return pl.DataFrame()


def merge_kline_and_metrics(kline_df, metrics_df, symbol):
    """
    åˆå¹¶Kçº¿æ•°æ®å’ŒMetricsæ•°æ®ï¼Œå¤„ç†ç¼ºå¤±æ•°æ®
    
    å‚æ•°:
        kline_df: åŒ…å«Kçº¿æ•°æ®çš„DataFrame
        metrics_df: åŒ…å«Metricsæ•°æ®çš„DataFrame
        symbol: äº¤æ˜“å¯¹ç¬¦å·
    
    è¿”å›:
        merged_df: åˆå¹¶åçš„æ•°æ®
        warning_dict: è­¦å‘Šä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæ²¡æœ‰ç¼ºå¤±æ•°æ®åˆ™ä¸ºç©ºå­—å…¸
    """
    import polars as pl
    
    # ç›´æ¥ä½¿ç”¨ä¹‹å‰è·å–çš„æ•°æ®æ¡†ï¼Œç»Ÿä¸€æ—¶é—´æ ¼å¼
    printLog(f"Kçº¿æ•°æ®: {len(kline_df)} è¡Œ, {list(kline_df.columns)}", level="debug")
    printLog(f"Metricsæ•°æ®: {len(metrics_df)} è¡Œ, {list(metrics_df.columns)}", level="debug")
    
    # ç¡®ä¿æ—¶é—´åˆ—éƒ½æœ‰æ­£ç¡®çš„æ—¶åŒºä¿¡æ¯å’Œç›¸åŒçš„æ—¶é—´ç²¾åº¦
    # è½¬æ¢kline_dfçš„candle_end_timeä¸ºUTCæ—¶åŒºçš„datetimeæ ¼å¼ï¼ˆè½¬æ¢ä¸ºå¾®ç§’ç²¾åº¦ï¼‰
    kline_df = kline_df.with_columns(
        candle_end_time_dt=pl.col("candle_end_time").dt.replace_time_zone("UTC").dt.cast_time_unit("us")
    )
    
    # è½¬æ¢metrics_dfçš„timestampä¸ºUTCæ—¶åŒºçš„datetimeæ ¼å¼ï¼ˆç¡®ä¿å¾®ç§’ç²¾åº¦ï¼‰
    metrics_df = metrics_df.with_columns(
        timestamp_dt=pl.col("timestamp").dt.replace_time_zone("UTC").dt.cast_time_unit("us")
    )
    
    # åˆå¹¶æ•°æ®
    merged_df = kline_df.join(
        metrics_df, 
        left_on="candle_end_time_dt", 
        right_on="timestamp_dt", 
        how="left"
    )
    
    # æŸ¥çœ‹åˆå¹¶åçš„æ•°æ®ç»“æ„
    printLog(f"åˆå¹¶åæ•°æ®: {len(merged_df)} è¡Œ, {list(merged_df.columns)}", level="debug")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_metrics = merged_df.filter(pl.col("timestamp").is_null())
    
    warning_dict = {}
    if not missing_metrics.is_empty():
        printLog(f"å‘ç° {len(missing_metrics)} è¡Œç¼ºå¤±Metricsæ•°æ®",level="run")
        
        # è¾“å‡ºè­¦å‘Šä¿¡æ¯åˆ°warning.json
        # å°†datetimeè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä»¥ä¾¿JSONåºåˆ—åŒ–
        missing_timestamps = missing_metrics.with_columns(
            pl.col("candle_end_time").dt.strftime("%Y-%m-%dT%H:%M:%S.%f%z").alias("candle_end_time")
        ).select("candle_end_time").to_dicts()
        
        warning_dict = {
            "symbol": symbol,
            "missing_count": len(missing_metrics),
            "missing_timestamps": missing_timestamps
        }
    
    # ç§»é™¤ä¸éœ€è¦çš„åˆ—
    metrics_columns = ["sum_open_interest", "sum_open_interest_value", "count_toptrader_long_short_ratio", "sum_toptrader_long_short_ratio", "count_long_short_ratio", "sum_taker_long_short_vol_ratio"]
    
    # åªåˆ é™¤å­˜åœ¨çš„åˆ—
    columns_to_drop = []
    if "symbol" in merged_df.columns:
        columns_to_drop.append("symbol")
    if "timestamp" in merged_df.columns:
        columns_to_drop.append("timestamp")
    if "timestamp_dt" in merged_df.columns:
        columns_to_drop.append("timestamp_dt")
    
    merged_df = merged_df.drop(columns_to_drop)
    
    # ä½¿ç”¨å‰å‘å¡«å……å¤„ç†ç¼ºå¤±çš„metricsæ•°æ®
    merged_df = merged_df.with_columns(
        [pl.col(col).forward_fill() for col in metrics_columns]
    )
    
    # ç§»é™¤ä¸´æ—¶æ—¶é—´åˆ—
    merged_df = merged_df.drop("candle_end_time_dt")
    
    if warning_dict:
        import json
        with open(WARNING_JSON, "w") as f:
            json.dump(warning_dict, f, indent=2)
        printLog(f"è­¦å‘Šä¿¡æ¯å·²ä¿å­˜åˆ° {WARNING_JSON}")
    
    printLog(f"æ•°æ®åˆå¹¶å®Œæˆ")
    printLog(f"   åˆå¹¶åè¡Œæ•°: {len(merged_df)}", level="run")
    printLog(f"   åˆå¹¶ååˆ—: {list(merged_df.columns)}", level="debug")

    return merged_df, warning_dict


async def main() -> None:

    Path(RootPath).mkdir(exist_ok=True)
    DATA_DIR.mkdir(exist_ok=True)
    OUTPUT_DIR.mkdir(exist_ok=True)
    PARSED_DATA_DIR.mkdir(exist_ok=True)

    """ä¸»å‡½æ•°"""
    
    # æ—¶é—´èŒƒå›´
    start_date = TEST_START_DATE
    end_date = TEST_END_DATE
    
    # æµ‹è¯•å•ä¸ªè´§å¸å¯¹
    test_symbol = TEST_SYMBOL
    
    try:
        # åˆ›å»ºoutputç›®å½•ç”¨äºä¿å­˜å›¾ç‰‡
        output_dir = OUTPUT_DIR
        output_dir.mkdir(exist_ok=True)
        
        # 1. è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®ï¼ˆé‡é‡‡æ ·åˆ°5åˆ†é’Ÿï¼‰
        kline_df = await get_kline_dataframe(
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            frequency="5m"
        )
        
        if not kline_df.is_empty():
            printLog(f"   è¡Œæ•°: {len(kline_df)}", level="debug")
            printLog(f"   åˆ—: {list(kline_df.columns)}", level="debug")
            
            # # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
            # printLog(f"ç»˜åˆ¶ {test_symbol} çš„Kçº¿æ•°æ®...")
            # save_path = output_dir / f"{test_symbol}_close_5m_{start_date}_{end_date}.png"
            # plot_dataframe(kline_df, data_type='kline', symbol=test_symbol, save_path=save_path)
        
        # 2. è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®
        metrics_df = await get_metrics_dataframe(
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
        )
        
        if not metrics_df.is_empty():
            printLog(f"   è¡Œæ•°: {len(metrics_df)}", level="debug")
            printLog(f"   åˆ—: {list(metrics_df.columns)}", level="debug")

            # ç»˜åˆ¶Metricsæ•°æ®
            # printLog(f"ç»˜åˆ¶ {test_symbol} çš„Metricsæ•°æ®...")
            # save_path = output_dir / f"{test_symbol}_metrics_{start_date}_{end_date}.png"
            # plot_dataframe(metrics_df, data_type='metrics', symbol=test_symbol, save_path=save_path)

        # å°†kline_dfå’Œmetrics_dfåˆå¹¶ï¼Œå¤„ç†ç¼ºå¤±æ•°æ®
        if not kline_df.is_empty() and not metrics_df.is_empty():
            printLog(f"\nåˆå¹¶ {test_symbol} çš„Kçº¿æ•°æ®å’ŒMetricsæ•°æ®...")
            
            # è°ƒç”¨å‡½æ•°åˆå¹¶æ•°æ®
            merged_df, warning_dict = merge_kline_and_metrics(kline_df, metrics_df, test_symbol)
            
            # ä¿å­˜è­¦å‘Šä¿¡æ¯åˆ°warning.json
            
            # ä¿å­˜åˆå¹¶åçš„æ•°æ®
            # merged_df.write_csv("./merged_data.csv")
            # printLog(f"   âœ… åˆå¹¶åçš„æ•°æ®å·²ä¿å­˜åˆ° ./merged_data.csv")

        printLog("\næ‰€æœ‰åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        printLog(f"ç¨‹åºè¿è¡Œå¤±è´¥: {e}", level="error")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    setPath("./DownLoadData")
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        printLog("\nâ¹ï¸  ç¨‹åºå·²è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        printLog(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}", level="error")
        import traceback
        traceback.print_exc()