#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆåŠŸèƒ½ï¼š
1. è·å–æ‰€æœ‰äº¤æ˜“å¯¹
2. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½Kçº¿ã€ç”Ÿæˆå…¨æ¯ã€é—´éš™æ£€æµ‹ä¿®å¤å¹¶è¿”å›DataFrame
3. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½metricsæ•°æ®å¹¶è¿”å›DataFrame
"""

# æ·»åŠ å½“å‰é¡¹ç›®çš„srcç›®å½•åˆ°Pythonè·¯å¾„
import sys
from pathlib import Path

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
project_root = Path(__file__).resolve().parent.parent
# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(project_root / "src"))

import asyncio
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import polars as pl
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib import style

# å…¨å±€æ•°æ®è·¯å¾„å¸¸é‡
BASE_DATA_PATH = "data"

# è®¾ç½®matplotlibæ ·å¼
style.use('seaborn-v0_8-darkgrid')

from bdt_common.constants import HTTP_TIMEOUT_SEC
from bdt_common.enums import DataFrequency, DataType, TradeType
from bdt_common.network import create_aiohttp_session
from bdt_common.polars_utils import execute_polars_batch
from bhds.aws.client import create_aws_client_from_config
from bhds.aws.downloader import AwsDownloader
from bhds.aws.checksum import ChecksumVerifier
from bhds.aws.local import AwsDataFileManager
from bhds.aws.parser import create_aws_parser
from bhds.holo_kline.merger import Holo1mKlineMerger
from bhds.holo_kline.gap_detector import HoloKlineGapDetector
from bhds.holo_kline.splitter import HoloKlineSplitter
from bhds.holo_kline.resampler import HoloKlineResampler


async def get_all_um_symbols(http_proxy: str) -> List[str]:
    """
    è·å–æ‰€æœ‰UMäº¤æ˜“å¯¹
    
    Args:
        http_proxy: HTTPä»£ç†
    
    Returns:
        æ‰€æœ‰UMäº¤æ˜“å¯¹åˆ—è¡¨
    """
    async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
        client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.kline,
            data_freq=DataFrequency.daily,
            time_interval="1m",
            session=session,
            http_proxy=http_proxy
        )
        symbols = await client.list_symbols()
        return symbols


def filter_files_by_time_range(files: List[Path], start_date: str, end_date: str) -> List[Path]:
    """
    ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ä»¶
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        ç­›é€‰åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # åˆ†ç¦»zipæ–‡ä»¶å’ŒCHECKSUMæ–‡ä»¶
    zip_files = [f for f in files if f.name.endswith('.zip')]
    checksum_files = [f for f in files if f.name == 'CHECKSUM']
    
    filtered_files = []
    
    # å…ˆè¾“å‡ºæ‰€æœ‰è·å–åˆ°çš„zipæ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"  å…±è·å–åˆ° {len(zip_files)} ä¸ªzipæ–‡ä»¶, {len(checksum_files)} ä¸ªCHECKSUMæ–‡ä»¶")
    if zip_files:
        print(f"  æœ€æ–°çš„5ä¸ªæ–‡ä»¶: {', '.join([f.name for f in sorted(zip_files)[-5:]])}")
    
    # æ—¥æœŸåŒ¹é…é€»è¾‘ - å¤„ç†zipæ–‡ä»¶
    for file_path in zip_files:
        # æ–‡ä»¶åæ ¼å¼ï¼šSYMBOL-TIME_INTERVAL-YYYY-MM-DD.zip
        # æˆ–ï¼šSYMBOL-YYYY-MM-DD.zip
        filename = file_path.name
        
        # æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸéƒ¨åˆ†
        import re
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', filename)
        if date_match:
            file_date = date_match.group()
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
            if start_date <= file_date <= end_date:
                filtered_files.append(file_path)
    
    # å°†CHECKSUMæ–‡ä»¶æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
    filtered_files.extend(checksum_files)
    
    return filtered_files


async def _download_single_symbol_data(
    http_proxy: str,
    symbol: str,
    data_dir: Path,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    ä¸‹è½½å•ä¸ªç¬¦å·çš„æŒ‡å®šç±»å‹æ•°æ®
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        data_type: æ•°æ®ç±»å‹ï¼ˆklineæˆ–metricsï¼‰
        time_interval: Kçº¿æ—¶é—´é—´éš”ï¼ˆmetricsä¸éœ€è¦ï¼‰
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ˜¯å¦æˆåŠŸä¸‹è½½
    """
    try:
        downloader = AwsDownloader(local_dir=data_dir, http_proxy=http_proxy, verbose=True)
        verifier = ChecksumVerifier(delete_mismatch=False)
        
        async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = create_aws_client_from_config(
                trade_type=TradeType.um_futures,
                data_type=data_type,
                data_freq=DataFrequency.daily,
                time_interval=time_interval,
                session=session,
                http_proxy=http_proxy
            )
            
            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = await client.list_data_files(symbol)
            range_files = filter_files_by_time_range(files, start_date, end_date)
            
            if not range_files:
                print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° {symbol} çš„{data_type.value}æ–‡ä»¶")
                return False
            
            # ä¸‹è½½æ–‡ä»¶
            print(f"ğŸ“¥ ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®...")
            print(f"  ä¸‹è½½æ–‡ä»¶åˆ—è¡¨ ({len(range_files)} ä¸ª):")
            for file in range_files:
                print(f"    - {file.name}")
            await downloader.aws_download(range_files)
            
            # éªŒè¯æ–‡ä»¶
            data_type_path = f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
            if data_type == DataType.kline:
                data_type_path += f"/{time_interval}"
            
            symbol_dir = data_dir / data_type_path
            manager = AwsDataFileManager(symbol_dir)
            unverified_files = manager.get_unverified_files()
            
            if unverified_files:
                results = verifier.verify_files(unverified_files)
                print(f"âœ… éªŒè¯å®Œæˆ: {results['success']} ä¸ªæˆåŠŸ, {results['failed']} ä¸ªå¤±è´¥")
                if results['failed'] > 0:
                    print(f"âŒ éªŒè¯å¤±è´¥è¯¦æƒ…: {results['errors']}")
                return results['failed'] == 0
            
            return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®å¤±è´¥: {e}")
        return False

async def _check_data_exists(
    data_dir: Path,
    symbol: str,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    æ£€æŸ¥æŒ‡å®šç±»å‹çš„æ•°æ®æ˜¯å¦å·²ç»ä¸‹è½½å¹¶éªŒè¯
    
    Args:
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        symbol: å¸å¯¹
        data_type: æ•°æ®ç±»å‹
        time_interval: Kçº¿æ—¶é—´é—´éš”
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ•°æ®æ˜¯å¦å­˜åœ¨ä¸”å·²éªŒè¯
    """
    from datetime import datetime, timedelta
    import re
    
    data_type_path = f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
    if data_type == DataType.kline:
        data_type_path += f"/{time_interval}"
    
    symbol_dir = data_dir / data_type_path
    if not symbol_dir.exists():
        return False
    
    manager = AwsDataFileManager(symbol_dir)
    verified_files = manager.get_verified_files()
    
    if not verified_files:
        return False
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    # æ”¶é›†æ‰€æœ‰å·²éªŒè¯æ–‡ä»¶çš„æ—¥æœŸ
    verified_dates = set()
    for file_path in verified_files:
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', file_path.name)
        if date_match:
            verified_dates.add(date_match.group())
    
    # æ‰¾å‡ºæ—¶é—´èŒƒå›´å†…ç¼ºå°‘çš„æ—¥æœŸ
    missing_dates = []
    current_dt = start_dt
    while current_dt <= end_dt:
        current_date_str = current_dt.strftime('%Y-%m-%d')
        if current_date_str not in verified_dates:
            missing_dates.append(current_date_str)
        current_dt += timedelta(days=1)
    
    # å¦‚æœæœ‰ç¼ºå°‘çš„æ—¥æœŸï¼Œæ‰“å°æ—¥å¿—å¹¶è¿”å›False
    if missing_dates:
        print(f"  âš ï¸  ç¼ºå°‘ä»¥ä¸‹æ—¥æœŸçš„æ•°æ®: {', '.join(missing_dates[:5])}{'...' if len(missing_dates) > 5 else ''}")
        return False
    
    return True  # æ‰€æœ‰æ—¥æœŸçš„æ•°æ®éƒ½å­˜åœ¨

async def get_kline_dataframe(
    http_proxy: str,
    symbol: str,
    start_date: str,
    end_date: str,
    data_dir: Path,
    parsed_data_dir: Path,
    time_interval: str = "1m",
    frequency: str = "1h"
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        time_interval: Kçº¿æ—¶é—´é—´éš”
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        Kçº¿æ•°æ®çš„DataFrame
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        data_dir=data_dir,
        symbol=symbol,
        data_type=DataType.kline,
        time_interval=time_interval,
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_dir=data_dir,
            data_type=DataType.kline,
            time_interval=time_interval,
            start_date=start_date,
            end_date=end_date
        )
    else:
        print(f"âœ… {symbol} çš„Kçº¿æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    # è§£ææ•°æ®
    parse_downloaded_data(
        data_dir=data_dir,
        symbols=[symbol],
        time_interval=time_interval,
        parsed_data_dir=parsed_data_dir,
        start_date=start_date,
        end_date=end_date
    )
    
    # ç”Ÿæˆå…¨æ¯Kçº¿
    import tempfile
    with tempfile.TemporaryDirectory(prefix="um_holo_") as temp_dir:
        temp_path = Path(temp_dir)
        
        # ç”Ÿæˆå…¨æ¯Kçº¿
        holo_files = generate_holo_klines(parsed_data_dir, TradeType.um_futures, temp_path, symbols=[symbol])
        
        if not holo_files:
            print(f"âŒ æ— æ³•ç”Ÿæˆ {symbol} çš„å…¨æ¯Kçº¿")
            return pl.DataFrame()
        
        # æ£€æµ‹å’Œå¤„ç†é—´éš™
        # symbols_with_gaps, _ = detect_and_process_gaps(holo_files, start_date=start_date, end_date=end_date)
        
        # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
        resampled_dir = temp_path / "resampled"
        resampled_dir.mkdir(parents=True, exist_ok=True)
        resampled_files = resample_holo_klines(holo_files, resampled_dir, frequency)
        
        # è·å–æœ€ç»ˆDataFrame
        result_df = get_final_dataframe(resampled_files, symbol)
        
        # å°†ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶
        csv_path = Path("./tempKlines.csv")
        result_df.write_csv(csv_path)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
        
        return result_df

def plot_dataframe(
    df: pl.DataFrame,
    data_type: Optional[str] = None,
    symbol: Optional[str] = None,
    save_path: Optional[Path] = None,
    figsize: Tuple[int, int] = (12, 6)
) -> None:
    """
    ç»˜åˆ¶DataFrameæ•°æ®çš„æŠ˜çº¿å›¾
    
    Args:
        df: è¦ç»˜åˆ¶çš„DataFrame
        data_type: æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'kline'æˆ–'metrics'ï¼Œå¦‚æœä¸æä¾›å°†è‡ªåŠ¨æ£€æµ‹
        symbol: å¸å¯¹åç§°ï¼Œç”¨äºæ ‡é¢˜
        save_path: ä¿å­˜å›¾ç‰‡çš„è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™æ˜¾ç¤ºå›¾ç‰‡
        figsize: å›¾ç‰‡å°ºå¯¸
    """
    if df.is_empty():
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾")
        return
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
    if data_type is None:
        if 'close' in df.columns:
            data_type = 'kline'
        else:
            data_type = 'metrics'
    
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=figsize)
    
    if data_type == 'kline':
        # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
        if 'candle_begin_time' in df.columns and 'close' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['candle_begin_time'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('candle_begin_time').str.to_datetime())
            
            # ç»˜åˆ¶æŠ˜çº¿å›¾
            plt.plot(df['candle_begin_time'], df['close'], label='Close Price', color='blue', linewidth=1.5)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.ylabel('Price')
            plt.title(f'{symbol} Close Price Chart' if symbol else 'Close Price Chart')
        else:
            print("âŒ Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'candle_begin_time' æˆ– 'close'")
            return
    
    elif data_type == 'metrics':
        # ç»˜åˆ¶metricsæ•°æ®
        if 'timestamp' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['timestamp'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('timestamp').str.to_datetime())
            
            # è·å–æ•°å€¼åˆ—
            numeric_columns = df.select([pl.col(pl.NUMERIC_DTYPES)]).columns
            
            # ç»˜åˆ¶æ‰€æœ‰æ•°å€¼åˆ—
            for col in numeric_columns:
                if col != 'timestamp':
                    plt.plot(df['timestamp'], df[col], label=col)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.title(f'{symbol} Metrics Chart' if symbol else 'Metrics Chart')
        else:
            print("âŒ Metricsæ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'timestamp'")
            return
    
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
        return
    
    # æ·»åŠ ç½‘æ ¼å’Œå›¾ä¾‹
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾ç‰‡
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
    else:
        plt.show()
    
    # å…³é—­å›¾å½¢
    plt.close()


async def get_metrics_dataframe(
    http_proxy: str,
    symbol: str,
    start_date: str,
    end_date: str,
    data_dir: Path
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
    
    Returns:
        Metricsæ•°æ®çš„DataFrame
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        data_dir=data_dir,
        symbol=symbol,
        data_type=DataType.metrics,
        time_interval="",
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_dir=data_dir,
            data_type=DataType.metrics,
            time_interval="",
            start_date=start_date,
            end_date=end_date
        )
    else:
        print(f"âœ… {symbol} çš„Metricsæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    metrics_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.metrics.value}/{symbol}"
    if metrics_symbol_dir.exists():
        manager = AwsDataFileManager(metrics_symbol_dir)
        verified_files = manager.get_verified_files()
        
        if verified_files:
            try:
                # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ä»¶
                filtered_files = verified_files
                if start_date and end_date:
                    filtered_files = filter_files_by_time_range(verified_files, start_date, end_date)
                    
                    # åˆ†ç¦»zipæ–‡ä»¶å’Œå…¶ä»–æ–‡ä»¶
                    filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                    if filtered_zip_files:
                        print(f"     ç­›é€‰å‡º {len(filtered_zip_files)} ä¸ªMetricsæ–‡ä»¶åœ¨ {start_date} - {end_date} èŒƒå›´å†…")
                    else:
                        print(f"     æ²¡æœ‰æ‰¾åˆ°åœ¨ {start_date} - {end_date} èŒƒå›´å†…çš„Metricsæ–‡ä»¶")
                        return pl.DataFrame()
                else:
                    print(f"     æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè§£ææ‰€æœ‰ {len(verified_files)} ä¸ªMetricsæ–‡ä»¶")
                
                # å°è¯•åˆ›å»ºmetricsè§£æå™¨
                metrics_parser = create_aws_parser(DataType.metrics)
                
                # åªå¤„ç†zipæ–‡ä»¶
                filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                if not filtered_zip_files:
                    print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯è§£æçš„Metrics zipæ–‡ä»¶")
                    return pl.DataFrame()
                
                # è¯»å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶å¹¶åˆå¹¶
                dfs = []
                for zip_file in filtered_zip_files:
                    try:
                        # ä»zipæ–‡ä»¶è¯»å–CSVæ•°æ®
                        df = metrics_parser.read_csv_from_zip(zip_file)
                        dfs.append(df)
                        print(f"     âœ… è§£æ {zip_file.name}")
                    except Exception as e:
                        print(f"     âŒ è§£æ {zip_file.name} å¤±è´¥: {e}")
                
                if dfs:
                    # åˆå¹¶æ‰€æœ‰DataFrame
                    combined_df = pl.concat(dfs)
                    print(f"âœ… æˆåŠŸè§£æ {symbol} çš„Metricsæ•°æ®ï¼Œå…± {len(combined_df)} è¡Œ")
                    return combined_df
                else:
                    print(f"âš ï¸  æ²¡æœ‰æˆåŠŸè§£æä»»ä½•Metricsæ–‡ä»¶")
                    return pl.DataFrame()
                    
            except Exception as e:
                print(f"âŒ è§£æ {symbol} çš„Metricsæ•°æ®å¤±è´¥: {e}")
                print("âš ï¸  å½“å‰ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒMetricsæ•°æ®çš„è§£æ")
                import traceback
                traceback.print_exc()
    
    return pl.DataFrame()


def parse_downloaded_data(
    data_dir: Path,
    symbols: List[str],
    time_interval: str,
    parsed_data_dir: Path,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> None:
    """
    è§£æä¸‹è½½çš„zipæ–‡ä»¶ä¸ºCSVæ ¼å¼
    
    Args:
        data_dir: ä¸‹è½½çš„æ•°æ®ç›®å½•
        symbols: å¸å¯¹åˆ—è¡¨
        time_interval: Kçº¿æ—¶é—´é—´éš”
        parsed_data_dir: è§£æåçš„æ•°æ®ä¿å­˜ç›®å½•
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰ï¼Œä»…è§£ææ­¤æ—¥æœŸä¹‹åçš„æ•°æ®
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰ï¼Œä»…è§£ææ­¤æ—¥æœŸä¹‹å‰çš„æ•°æ®
    """
    print(f"\nğŸ”„ è§£æä¸‹è½½çš„æ•°æ®...")
    
    kline_parser = create_aws_parser(DataType.kline)
    
    for symbol in symbols:
        print(f"   è§£æ {symbol}...")
        
        # è§£æKçº¿æ•°æ®
        kline_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
        if kline_symbol_dir.exists():
            manager = AwsDataFileManager(kline_symbol_dir)
            verified_files = manager.get_verified_files()
            
            if verified_files:
                # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ–‡ä»¶
                filtered_files = verified_files
                if start_date and end_date:
                    filtered_files = filter_files_by_time_range(verified_files, start_date, end_date)
                    
                    # åˆ†ç¦»zipæ–‡ä»¶å’Œå…¶ä»–æ–‡ä»¶
                    filtered_zip_files = [f for f in filtered_files if f.name.endswith('.zip')]
                    if filtered_zip_files:
                        print(f"     ç­›é€‰å‡º {len(filtered_zip_files)} ä¸ªæ–‡ä»¶åœ¨ {start_date} - {end_date} èŒƒå›´å†…")
                    else:
                        print(f"     æ²¡æœ‰æ‰¾åˆ°åœ¨ {start_date} - {end_date} èŒƒå›´å†…çš„æ–‡ä»¶")
                        continue
                else:
                    print(f"     æœªæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œè§£ææ‰€æœ‰ {len(verified_files)} ä¸ªæ–‡ä»¶")
                
                # ç¡®ä¿è§£æç›®å½•å­˜åœ¨ï¼ˆåŒ…å«data/å‰ç¼€ï¼‰
                symbol_parsed_dir = parsed_data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
                symbol_parsed_dir.mkdir(parents=True, exist_ok=True)
                
                # æ¸…ç†æ—§çš„CSVæ–‡ä»¶
                for csv_file in symbol_parsed_dir.glob("*.csv"):
                    csv_file.unlink()
                    print(f"     ğŸ—‘ï¸  åˆ é™¤æ—§çš„CSVæ–‡ä»¶: {csv_file.name}")
                
                # åªå¤„ç†zipæ–‡ä»¶
                for zip_file in [f for f in filtered_files if f.name.endswith('.zip')]:
                    try:
                        # ä»zipæ–‡ä»¶è¯»å–CSVæ•°æ®
                        df = kline_parser.read_csv_from_zip(zip_file)
                        
                        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
                        parquet_file = symbol_parsed_dir / f"{zip_file.stem}.parquet"
                        df.write_parquet(parquet_file)
                        print(f"     âœ… è§£æ {zip_file.name} -> {parquet_file.name}")
                    except Exception as e:
                        print(f"     âŒ è§£æ {zip_file.name} å¤±è´¥: {e}")
    
    print("âœ… æ•°æ®è§£æå®Œæˆ")


def generate_holo_klines(
    parsed_data_dir: Path,
    trade_type: TradeType,
    output_dir: Path,
    symbols: Optional[List[str]] = None
) -> List[Path]:
    """
    ç”Ÿæˆå…¨æ¯kçº¿
    
    Args:
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        trade_type: äº¤æ˜“ç±»å‹
        output_dir: è¾“å‡ºç›®å½•
        symbols: è¦å¤„ç†çš„ç¬¦å·åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç¬¦å·
    
    Returns:
        ç”Ÿæˆçš„å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
    """
    print(f"\nğŸ”„ ç”Ÿæˆå…¨æ¯kçº¿...")
    merger = Holo1mKlineMerger(
        trade_type=trade_type,
        base_dir=parsed_data_dir,
        include_vwap=True,
        include_funding=True,
    )
    
    # ç”ŸæˆæŒ‡å®šç¬¦å·çš„å…¨æ¯kçº¿
    lazy_frames = merger.generate_all(output_dir, target_symbols=symbols)
    if not lazy_frames:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„ç¬¦å·")
        return []
    
    # æ‰§è¡ŒPolarsæ‰¹å¤„ç†ä»¥ç”Ÿæˆæ–‡ä»¶
    execute_polars_batch(lazy_frames, "Collecting kline data")
    
    # è·å–ç”Ÿæˆçš„æ–‡ä»¶
    generated_files = list(output_dir.glob("*.parquet"))
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªå…¨æ¯kçº¿æ–‡ä»¶")
    return generated_files


def detect_and_process_gaps(
    holo_files: List[Path],
    min_days: int = 1,
    min_price_chg: float = 0.1,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> Tuple[int, int]:
    """
    æ£€æµ‹å’Œå¤„ç†é—´éš™
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        min_days: æœ€å°é—´éš™å¤©æ•°
        min_price_chg: æœ€å°ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        (æœ‰é—´éš™çš„ç¬¦å·æ•°, ç”Ÿæˆçš„åˆ†å‰²æ–‡ä»¶æ•°)
    """
    print(f"\nğŸ”„ æ£€æµ‹é—´éš™...")
    print(f"     Min days: {min_days}")
    print(f"     Min price change: {min_price_chg * 100}%")
    
    detector = HoloKlineGapDetector(min_days, min_price_chg)
    splitter = HoloKlineSplitter(prefix="SP")
    
    # ç”Ÿæˆé—´éš™æ£€æµ‹ä»»åŠ¡
    gap_tasks = [detector.detect(file_path) for file_path in holo_files]
    gap_results = execute_polars_batch(gap_tasks, "Detecting gaps", return_results=True)
    
    symbols_with_gaps = 0
    total_splits = 0
    
    # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
    has_time_filter = False
    filter_start = None
    filter_end = None
    
    if start_date and end_date:
        from datetime import datetime
        # è§£ææ—¥æœŸå­—ç¬¦ä¸²
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # è½¬æ¢ä¸ºPolars datetimeå¯¹è±¡
        filter_start = pl.datetime(
            year=start_dt.year, month=start_dt.month, day=start_dt.day,
            time_zone="UTC"
        )
        filter_end = pl.datetime(
            year=end_dt.year, month=end_dt.month, day=end_dt.day,
            time_zone="UTC"
        ) + pl.duration(days=1) - pl.duration(microseconds=1)
        
        has_time_filter = True
    
    # å¤„ç†é—´éš™ç»“æœ
    for file_path, gaps_df in zip(holo_files, gap_results):
        if len(gaps_df) > 0:
            symbol = file_path.stem
            symbols_with_gaps += 1
            
            print(f"\nğŸ” {symbol} - {len(gaps_df)} gap(s)")
            print("-" * 40)
            
            # è¿‡æ»¤å‡ºæŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„é—´éš™
            if has_time_filter:
                gaps_df = gaps_df.filter(
                    (pl.col("prev_begin_time") >= filter_start) & 
                    (pl.col("candle_begin_time") <= filter_end)
                )
            
            for gap in gaps_df.sort("time_diff", descending=True).iter_rows(named=True):
                print(f"  {gap['prev_begin_time']} â†’ {gap['candle_begin_time']}")
                print(f"  Duration: {gap['time_diff']}, Change: {gap['price_change']:.2%}")
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„é—´éš™åˆ†å‰²kçº¿æ•°æ®
            print(f"  åˆ†å‰² {symbol}...")
            split_files = splitter.split_file(file_path, gaps_df)
            total_splits += len(split_files)
            
            for split_file in split_files:
                seg_df = pl.read_parquet(split_file)
                min_begin_time = seg_df["candle_begin_time"].min()
                max_begin_time = seg_df["candle_begin_time"].max()
                print(f"    {split_file.name}: {len(seg_df)} è¡Œ, {min_begin_time} åˆ° {max_begin_time}")
    
    print(f"\nğŸ“ˆ æ€»ç»“: {symbols_with_gaps}/{len(holo_files)} ä¸ªç¬¦å·æœ‰é—´éš™")
    print(f"         ç”Ÿæˆäº† {total_splits} ä¸ªåˆ†å‰²æ–‡ä»¶")
    
    return symbols_with_gaps, total_splits


def resample_holo_klines(holo_files: List[Path], output_dir: Path, frequency: str = "1h") -> List[Path]:
    """
    å°†å…¨æ¯kçº¿é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
    """
    print(f"\nğŸ”„ å°†æ•°æ®é‡é‡‡æ ·åˆ°{frequency}...")
    
    # åˆå§‹åŒ–é‡é‡‡æ ·å™¨
    resampler = HoloKlineResampler(resample_interval=frequency)
    
    resampled_files = []
    for file_path in holo_files:
        symbol = file_path.stem
        output_file = output_dir / f"{symbol}_{frequency}.parquet"
        
        try:
            # è¯»å–å…¨æ¯kçº¿æ•°æ®
            df = pl.read_parquet(file_path)
            
            # å°†DataFrameè½¬æ¢ä¸ºLazyFrame
            ldf = df.lazy()
            
            # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
            resampled_ldf = resampler.resample(ldf)
            
            # è®¡ç®—å¹¶ä¿å­˜é‡é‡‡æ ·åçš„æ•°æ®
            resampled_df = resampled_ldf.collect()
            resampled_df.write_parquet(output_file)
            resampled_files.append(output_file)
            
            print(f"   âœ… {symbol}: {len(df)} è¡Œ â†’ {len(resampled_df)} è¡Œ")
        except Exception as e:
            print(f"   âŒ {symbol}: é‡é‡‡æ ·å¤±è´¥ - {e}")
            import traceback
            traceback.print_exc()
    
    print(f"âœ… æˆåŠŸé‡é‡‡æ · {len(resampled_files)} ä¸ªæ–‡ä»¶åˆ°1h")
    return resampled_files


def get_final_dataframe(resampled_files: List[Path], symbol: str) -> pl.DataFrame:
    """
    è·å–æœ€ç»ˆçš„DataFrame
    
    Args:
        resampled_files: é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
        symbol: è¦è·å–çš„ç¬¦å·
    
    Returns:
        æœ€ç»ˆçš„DataFrame
    """
    for file_path in resampled_files:
        if file_path.stem.startswith(symbol):
            return pl.read_parquet(file_path)
    
    print(f"âŒ æ²¡æœ‰æ‰¾åˆ° {symbol} çš„é‡é‡‡æ ·æ–‡ä»¶")
    return pl.DataFrame()


async def main() -> None:
    """ä¸»å‡½æ•°"""
    # é…ç½®
    http_proxy = "http://127.0.0.1:7890"  # 7890ä»£ç†
    data_dir = Path("d:/Codes/binance_datatool-main/data")  # æ•°æ®ä¿å­˜ç›®å½•
    parsed_data_dir = Path("d:/Codes/binance_datatool-main/parsed_data")  # è§£æåçš„æ•°æ®ç›®å½•
    
    # æ—¶é—´èŒƒå›´
    start_date = "2026-01-01"
    end_date = "2026-01-10"
    
    # æµ‹è¯•å•ä¸ªè´§å¸å¯¹
    test_symbol = "BTCUSDT"
    
    try:
        # åˆ›å»ºoutputç›®å½•ç”¨äºä¿å­˜å›¾ç‰‡
        output_dir = Path("d:/Codes/binance_datatool-main/output")
        output_dir.mkdir(exist_ok=True)
        
        # 1. è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®ï¼ˆé‡é‡‡æ ·åˆ°5åˆ†é’Ÿï¼‰
        print(f"\nğŸ“Š è·å– {test_symbol} çš„Kçº¿æ•°æ®ï¼ˆ{start_date} ~ {end_date}ï¼‰...")
        kline_df = await get_kline_dataframe(
            http_proxy=http_proxy,
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            data_dir=data_dir,
            parsed_data_dir=parsed_data_dir,
            frequency="5m"
        )
        
        if not kline_df.is_empty():
            print(f"âœ… æˆåŠŸè·å– {test_symbol} çš„Kçº¿æ•°æ®")
            print(f"   è¡Œæ•°: {len(kline_df)}")
            print(f"   åˆ—: {list(kline_df.columns)}")
            
            # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
            print(f"ğŸ“Š ç»˜åˆ¶ {test_symbol} çš„Kçº¿æ•°æ®...")
            save_path = output_dir / f"{test_symbol}_close_5m_{start_date}_{end_date}.png"
            plot_dataframe(kline_df, data_type='kline', symbol=test_symbol, save_path=save_path)
        
        # 2. è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®
        print(f"\nğŸ“Š è·å– {test_symbol} çš„Metricsæ•°æ®ï¼ˆ{start_date} ~ {end_date}ï¼‰...")
        metrics_df = await get_metrics_dataframe(
            http_proxy=http_proxy,
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            data_dir=data_dir
        )
        
        if not metrics_df.is_empty():
            print(f"âœ… æˆåŠŸè·å– {test_symbol} çš„Metricsæ•°æ®")
            print(f"   è¡Œæ•°: {len(metrics_df)}")
            print(f"   åˆ—: {list(metrics_df.columns)}")
            metrics_df.to_csv("./tempMetrics.csv", index=False)

            # ç»˜åˆ¶Metricsæ•°æ®
            print(f"ğŸ“Š ç»˜åˆ¶ {test_symbol} çš„Metricsæ•°æ®...")
            save_path = output_dir / f"{test_symbol}_metrics_{start_date}_{end_date}.png"
            plot_dataframe(metrics_df, data_type='metrics', symbol=test_symbol, save_path=save_path)
        
        print("\nâœ… æ‰€æœ‰åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç¨‹åºå·²è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()