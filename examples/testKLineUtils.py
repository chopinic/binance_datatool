#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•´åˆåŠŸèƒ½ï¼š
1. è·å–æ‰€æœ‰äº¤æ˜“å¯¹
2. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½Kçº¿ã€ç”Ÿæˆå…¨æ¯ã€é—´éš™æ£€æµ‹ä¿®å¤å¹¶è¿”å›DataFrame
3. ç»™å•ä¸ªè´§å¸å¯¹å’Œæ—¶é—´æ®µï¼Œæ£€æµ‹å¹¶ä¸‹è½½metricsæ•°æ®å¹¶è¿”å›DataFrame
"""

import asyncio
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import polars as pl
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib import style

# å…¨å±€æ•°æ®è·¯å¾„å¸¸é‡
BASE_DATA_PATH = "data"

# è®¾ç½®matplotlibæ ·å¼
style.use('seaborn-v0_8-darkgrid')

from bdt_common.constants import HTTP_TIMEOUT_SEC
from bdt_common.enums import DataFrequency, DataType, TradeType
from bdt_common.network import create_aiohttp_session
from bdt_common.polars_utils import execute_polars_batch
from bhds.aws.client import create_aws_client_from_config
from bhds.aws.downloader import AwsDownloader
from bhds.aws.checksum import ChecksumVerifier
from bhds.aws.local import AwsDataFileManager
from bhds.aws.parser import create_aws_parser
from bhds.holo_kline.merger import Holo1mKlineMerger
from bhds.holo_kline.gap_detector import HoloKlineGapDetector
from bhds.holo_kline.splitter import HoloKlineSplitter
from bhds.holo_kline.resampler import HoloKlineResampler


async def get_all_um_symbols(http_proxy: str) -> List[str]:
    """
    è·å–æ‰€æœ‰UMäº¤æ˜“å¯¹
    
    Args:
        http_proxy: HTTPä»£ç†
    
    Returns:
        æ‰€æœ‰UMäº¤æ˜“å¯¹åˆ—è¡¨
    """
    async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
        client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.kline,
            data_freq=DataFrequency.daily,
            time_interval="1m",
            session=session,
            http_proxy=http_proxy
        )
        symbols = await client.list_symbols()
        return symbols


async def filter_files_by_time_range(files: List[Path], start_date: str, end_date: str) -> List[Path]:
    """
    ç­›é€‰æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ä»¶
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        start_date: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        ç­›é€‰åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # è¿‡æ»¤æ‰CHECKSUMæ–‡ä»¶ï¼Œåªå¤„ç†zipæ–‡ä»¶
    zip_files = [f for f in files if f.name.endswith('.zip')]
    
    filtered_files = []
    
    # å…ˆè¾“å‡ºæ‰€æœ‰è·å–åˆ°çš„zipæ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"  å…±è·å–åˆ° {len(zip_files)} ä¸ªzipæ–‡ä»¶")
    if zip_files:
        print(f"  æœ€æ–°çš„5ä¸ªæ–‡ä»¶: {', '.join([f.name for f in sorted(zip_files)[-5:]])}")
    
    # æ—¥æœŸåŒ¹é…é€»è¾‘
    for file_path in zip_files:
        # æ–‡ä»¶åæ ¼å¼ï¼šSYMBOL-TIME_INTERVAL-YYYY-MM-DD.zip
        # æˆ–ï¼šSYMBOL-YYYY-MM-DD.zip
        filename = file_path.name
        
        # æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸéƒ¨åˆ†
        import re
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', filename)
        if date_match:
            file_date = date_match.group()
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
            if start_date <= file_date <= end_date:
                filtered_files.append(file_path)
    
    return filtered_files


async def _download_single_symbol_data(
    http_proxy: str,
    symbol: str,
    data_dir: Path,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    ä¸‹è½½å•ä¸ªç¬¦å·çš„æŒ‡å®šç±»å‹æ•°æ®
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        data_type: æ•°æ®ç±»å‹ï¼ˆklineæˆ–metricsï¼‰
        time_interval: Kçº¿æ—¶é—´é—´éš”ï¼ˆmetricsä¸éœ€è¦ï¼‰
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ˜¯å¦æˆåŠŸä¸‹è½½
    """
    try:
        downloader = AwsDownloader(local_dir=data_dir, http_proxy=http_proxy, verbose=True)
        verifier = ChecksumVerifier(delete_mismatch=False)
        
        async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = create_aws_client_from_config(
                trade_type=TradeType.um_futures,
                data_type=data_type,
                data_freq=DataFrequency.daily,
                time_interval=time_interval,
                session=session,
                http_proxy=http_proxy
            )
            
            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = await client.list_data_files(symbol)
            range_files = await filter_files_by_time_range(files, start_date, end_date)
            
            if not range_files:
                print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ° {symbol} çš„{data_type.value}æ–‡ä»¶")
                return False
            
            # ä¸‹è½½æ–‡ä»¶
            print(f"ğŸ“¥ ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®...")
            await downloader.aws_download(range_files)
            
            # éªŒè¯æ–‡ä»¶
            data_type_path = f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
            if data_type == DataType.kline:
                data_type_path += f"/{time_interval}"
            
            symbol_dir = data_dir / data_type_path
            manager = AwsDataFileManager(symbol_dir)
            unverified_files = manager.get_unverified_files()
            
            if unverified_files:
                results = verifier.verify_files(unverified_files)
                print(f"âœ… éªŒè¯å®Œæˆ: {results['success']} ä¸ªæˆåŠŸ, {results['failed']} ä¸ªå¤±è´¥")
                return results['failed'] == 0
            
            return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½ {symbol} çš„{data_type.value}æ•°æ®å¤±è´¥: {e}")
        return False

async def _check_data_exists(
    data_dir: Path,
    symbol: str,
    data_type: DataType,
    time_interval: str,
    start_date: str,
    end_date: str
) -> bool:
    """
    æ£€æŸ¥æŒ‡å®šç±»å‹çš„æ•°æ®æ˜¯å¦å·²ç»ä¸‹è½½å¹¶éªŒè¯
    
    Args:
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        symbol: å¸å¯¹
        data_type: æ•°æ®ç±»å‹
        time_interval: Kçº¿æ—¶é—´é—´éš”
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        æ•°æ®æ˜¯å¦å­˜åœ¨ä¸”å·²éªŒè¯
    """
    data_type_path = f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{data_type.value}/{symbol}"
    if data_type == DataType.kline:
        data_type_path += f"/{time_interval}"
    
    symbol_dir = data_dir / data_type_path
    if not symbol_dir.exists():
        return False
    
    manager = AwsDataFileManager(symbol_dir)
    verified_files = manager.get_verified_files()
    
    if not verified_files:
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦åˆæ—¶é—´èŒƒå›´çš„æ–‡ä»¶
    for file_path in verified_files:
        import re
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', file_path.name)
        if date_match:
            file_date = date_match.group()
            if start_date <= file_date <= end_date:
                return True
    
    return False

async def get_kline_dataframe(
    http_proxy: str,
    symbol: str,
    start_date: str,
    end_date: str,
    data_dir: Path,
    parsed_data_dir: Path,
    time_interval: str = "1m",
    frequency: str = "1h"
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        time_interval: Kçº¿æ—¶é—´é—´éš”
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        Kçº¿æ•°æ®çš„DataFrame
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        data_dir=data_dir,
        symbol=symbol,
        data_type=DataType.kline,
        time_interval=time_interval,
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_dir=data_dir,
            data_type=DataType.kline,
            time_interval=time_interval,
            start_date=start_date,
            end_date=end_date
        )
    else:
        print(f"âœ… {symbol} çš„Kçº¿æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    # è§£ææ•°æ®
    parse_downloaded_data(
        data_dir=data_dir,
        symbols=[symbol],
        time_interval=time_interval,
        parsed_data_dir=parsed_data_dir
    )
    
    # ç”Ÿæˆå…¨æ¯Kçº¿
    import tempfile
    with tempfile.TemporaryDirectory(prefix="um_holo_") as temp_dir:
        temp_path = Path(temp_dir)
        
        # ç”Ÿæˆå…¨æ¯Kçº¿
        holo_files = generate_holo_klines(parsed_data_dir, TradeType.um_futures, temp_path)
        
        if not holo_files:
            print(f"âŒ æ— æ³•ç”Ÿæˆ {symbol} çš„å…¨æ¯Kçº¿")
            return pl.DataFrame()
        
        # æ£€æµ‹å’Œå¤„ç†é—´éš™
        symbols_with_gaps, _ = detect_and_process_gaps(holo_files)
        
        # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
        resampled_dir = temp_path / "resampled"
        resampled_dir.mkdir(parents=True, exist_ok=True)
        resampled_files = resample_holo_klines(holo_files, resampled_dir, frequency)
        
        # è·å–æœ€ç»ˆDataFrame
        return get_final_dataframe(resampled_files, symbol)

def plot_dataframe(
    df: pl.DataFrame,
    data_type: Optional[str] = None,
    symbol: Optional[str] = None,
    save_path: Optional[Path] = None,
    figsize: Tuple[int, int] = (12, 6)
) -> None:
    """
    ç»˜åˆ¶DataFrameæ•°æ®çš„æŠ˜çº¿å›¾
    
    Args:
        df: è¦ç»˜åˆ¶çš„DataFrame
        data_type: æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'kline'æˆ–'metrics'ï¼Œå¦‚æœä¸æä¾›å°†è‡ªåŠ¨æ£€æµ‹
        symbol: å¸å¯¹åç§°ï¼Œç”¨äºæ ‡é¢˜
        save_path: ä¿å­˜å›¾ç‰‡çš„è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™æ˜¾ç¤ºå›¾ç‰‡
        figsize: å›¾ç‰‡å°ºå¯¸
    """
    if df.is_empty():
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜å›¾")
        return
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
    if data_type is None:
        if 'close' in df.columns:
            data_type = 'kline'
        else:
            data_type = 'metrics'
    
    # åˆ›å»ºå›¾å½¢
    plt.figure(figsize=figsize)
    
    if data_type == 'kline':
        # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
        if 'candle_begin_time' in df.columns and 'close' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['candle_begin_time'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('candle_begin_time').str.to_datetime())
            
            # ç»˜åˆ¶æŠ˜çº¿å›¾
            plt.plot(df['candle_begin_time'], df['close'], label='Close Price', color='blue', linewidth=1.5)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.ylabel('Price')
            plt.title(f'{symbol} Close Price Chart' if symbol else 'Close Price Chart')
        else:
            print("âŒ Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'candle_begin_time' æˆ– 'close'")
            return
    
    elif data_type == 'metrics':
        # ç»˜åˆ¶metricsæ•°æ®
        if 'timestamp' in df.columns:
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            if df['timestamp'].dtype != pl.Datetime:
                df = df.with_columns(pl.col('timestamp').str.to_datetime())
            
            # è·å–æ•°å€¼åˆ—
            numeric_columns = df.select([pl.col(pl.NUMERIC_DTYPES)]).columns
            
            # ç»˜åˆ¶æ‰€æœ‰æ•°å€¼åˆ—
            for col in numeric_columns:
                if col != 'timestamp':
                    plt.plot(df['timestamp'], df[col], label=col)
            
            # è®¾ç½®xè½´æ—¥æœŸæ ¼å¼
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.xticks(rotation=45)
            
            plt.title(f'{symbol} Metrics Chart' if symbol else 'Metrics Chart')
        else:
            print("âŒ Metricsæ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: 'timestamp'")
            return
    
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
        return
    
    # æ·»åŠ ç½‘æ ¼å’Œå›¾ä¾‹
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾ç‰‡
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}")
    else:
        plt.show()
    
    # å…³é—­å›¾å½¢
    plt.close()


async def get_metrics_dataframe(
    http_proxy: str,
    symbol: str,
    start_date: str,
    end_date: str,
    data_dir: Path
) -> pl.DataFrame:
    """
    è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®DataFrame
    
    Args:
        http_proxy: HTTPä»£ç†
        symbol: å¸å¯¹
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
    
    Returns:
        Metricsæ•°æ®çš„DataFrame
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ä¸‹è½½
    data_exists = await _check_data_exists(
        data_dir=data_dir,
        symbol=symbol,
        data_type=DataType.metrics,
        time_interval="",
        start_date=start_date,
        end_date=end_date
    )
    
    if not data_exists:
        # ä¸‹è½½æ•°æ®
        await _download_single_symbol_data(
            http_proxy=http_proxy,
            symbol=symbol,
            data_dir=data_dir,
            data_type=DataType.metrics,
            time_interval="",
            start_date=start_date,
            end_date=end_date
        )
    else:
        print(f"âœ… {symbol} çš„Metricsæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
    
    # è§£æMetricsæ•°æ®ï¼ˆæ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬çš„create_aws_parserå¯èƒ½ä¸æ”¯æŒmetricsç±»å‹ï¼‰
    metrics_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.metrics.value}/{symbol}"
    if metrics_symbol_dir.exists():
        manager = AwsDataFileManager(metrics_symbol_dir)
        verified_files = manager.get_verified_files()
        
        if verified_files:
            try:
                # å°è¯•åˆ›å»ºmetricsè§£æå™¨
                metrics_parser = create_aws_parser(DataType.metrics)
                
                # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
                df = metrics_parser.read_csv_from_zip(verified_files[0])
                print(f"âœ… æˆåŠŸè§£æ {symbol} çš„Metricsæ•°æ®")
                return df
            except Exception as e:
                print(f"âŒ è§£æ {symbol} çš„Metricsæ•°æ®å¤±è´¥: {e}")
                print("âš ï¸  å½“å‰ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒMetricsæ•°æ®çš„è§£æ")
    
    return pl.DataFrame()


def parse_downloaded_data(
    data_dir: Path,
    symbols: List[str],
    time_interval: str,
    parsed_data_dir: Path
) -> None:
    """
    è§£æä¸‹è½½çš„zipæ–‡ä»¶ä¸ºCSVæ ¼å¼
    
    Args:
        data_dir: ä¸‹è½½çš„æ•°æ®ç›®å½•
        symbols: å¸å¯¹åˆ—è¡¨
        time_interval: Kçº¿æ—¶é—´é—´éš”
        parsed_data_dir: è§£æåçš„æ•°æ®ä¿å­˜ç›®å½•
    """
    print(f"\nğŸ”„ è§£æä¸‹è½½çš„æ•°æ®...")
    
    # åˆ›å»ºè§£æå™¨ï¼ˆä»…æ”¯æŒklineç±»å‹ï¼‰
    kline_parser = create_aws_parser(DataType.kline)
    
    for symbol in symbols:
        print(f"   è§£æ {symbol}...")
        
        # è§£æKçº¿æ•°æ®
        kline_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
        if kline_symbol_dir.exists():
            manager = AwsDataFileManager(kline_symbol_dir)
            verified_files = manager.get_verified_files()
            
            if verified_files:
                # ç¡®ä¿è§£æç›®å½•å­˜åœ¨ï¼ˆåŒ…å«data/å‰ç¼€ï¼‰
                symbol_parsed_dir = parsed_data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
                symbol_parsed_dir.mkdir(parents=True, exist_ok=True)
                
                # æ¸…ç†æ—§çš„CSVæ–‡ä»¶
                for csv_file in symbol_parsed_dir.glob("*.csv"):
                    csv_file.unlink()
                    print(f"     ğŸ—‘ï¸  åˆ é™¤æ—§çš„CSVæ–‡ä»¶: {csv_file.name}")
                
                for zip_file in verified_files:
                    try:
                        # ä»zipæ–‡ä»¶è¯»å–CSVæ•°æ®
                        df = kline_parser.read_csv_from_zip(zip_file)
                        
                        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
                        parquet_file = symbol_parsed_dir / f"{zip_file.stem}.parquet"
                        df.write_parquet(parquet_file)
                        print(f"     âœ… è§£æ {zip_file.name} -> {parquet_file.name}")
                    except Exception as e:
                        print(f"     âŒ è§£æ {zip_file.name} å¤±è´¥: {e}")
    
    print("âœ… æ•°æ®è§£æå®Œæˆ")


def generate_holo_klines(
    parsed_data_dir: Path,
    trade_type: TradeType,
    output_dir: Path
) -> List[Path]:
    """
    ç”Ÿæˆå…¨æ¯kçº¿
    
    Args:
        parsed_data_dir: è§£æåçš„æ•°æ®ç›®å½•
        trade_type: äº¤æ˜“ç±»å‹
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        ç”Ÿæˆçš„å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
    """
    print(f"\nğŸ”„ ç”Ÿæˆå…¨æ¯kçº¿...")
    merger = Holo1mKlineMerger(
        trade_type=trade_type,
        base_dir=parsed_data_dir,
        include_vwap=True,
        include_funding=True,
    )
    
    # ç”Ÿæˆæ‰€æœ‰ç¬¦å·çš„å…¨æ¯kçº¿
    lazy_frames = merger.generate_all(output_dir)
    if not lazy_frames:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„ç¬¦å·")
        return []
    
    # æ‰§è¡ŒPolarsæ‰¹å¤„ç†ä»¥ç”Ÿæˆæ–‡ä»¶
    execute_polars_batch(lazy_frames, "Collecting kline data")
    
    # è·å–ç”Ÿæˆçš„æ–‡ä»¶
    generated_files = list(output_dir.glob("*.parquet"))
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªå…¨æ¯kçº¿æ–‡ä»¶")
    return generated_files


def detect_and_process_gaps(
    holo_files: List[Path],
    min_days: int = 1,
    min_price_chg: float = 0.1
) -> Tuple[int, int]:
    """
    æ£€æµ‹å’Œå¤„ç†é—´éš™
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        min_days: æœ€å°é—´éš™å¤©æ•°
        min_price_chg: æœ€å°ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
    
    Returns:
        (æœ‰é—´éš™çš„ç¬¦å·æ•°, ç”Ÿæˆçš„åˆ†å‰²æ–‡ä»¶æ•°)
    """
    print(f"\nğŸ”„ æ£€æµ‹é—´éš™...")
    print(f"     Min days: {min_days}")
    print(f"     Min price change: {min_price_chg * 100}%")
    
    detector = HoloKlineGapDetector(min_days, min_price_chg)
    splitter = HoloKlineSplitter(prefix="SP")
    
    # ç”Ÿæˆé—´éš™æ£€æµ‹ä»»åŠ¡
    gap_tasks = [detector.detect(file_path) for file_path in holo_files]
    gap_results = execute_polars_batch(gap_tasks, "Detecting gaps", return_results=True)
    
    symbols_with_gaps = 0
    total_splits = 0
    
    # å¤„ç†é—´éš™ç»“æœ
    for file_path, gaps_df in zip(holo_files, gap_results):
        if len(gaps_df) > 0:
            symbol = file_path.stem
            symbols_with_gaps += 1
            
            print(f"\nğŸ” {symbol} - {len(gaps_df)} gap(s)")
            print("-" * 40)
            
            for gap in gaps_df.sort("time_diff", descending=True).iter_rows(named=True):
                print(f"  {gap['prev_begin_time']} â†’ {gap['candle_begin_time']}")
                print(f"  Duration: {gap['time_diff']}, Change: {gap['price_change']:.2%}")
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„é—´éš™åˆ†å‰²kçº¿æ•°æ®
            print(f"  åˆ†å‰² {symbol}...")
            split_files = splitter.split_file(file_path, gaps_df)
            total_splits += len(split_files)
            
            for split_file in split_files:
                seg_df = pl.read_parquet(split_file)
                min_begin_time = seg_df["candle_begin_time"].min()
                max_begin_time = seg_df["candle_begin_time"].max()
                print(f"    {split_file.name}: {len(seg_df)} è¡Œ, {min_begin_time} åˆ° {max_begin_time}")
    
    print(f"\nğŸ“ˆ æ€»ç»“: {symbols_with_gaps}/{len(holo_files)} ä¸ªç¬¦å·æœ‰é—´éš™")
    print(f"         ç”Ÿæˆäº† {total_splits} ä¸ªåˆ†å‰²æ–‡ä»¶")
    
    return symbols_with_gaps, total_splits


def resample_holo_klines(holo_files: List[Path], output_dir: Path, frequency: str = "1h") -> List[Path]:
    """
    å°†å…¨æ¯kçº¿é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
    
    Args:
        holo_files: å…¨æ¯kçº¿æ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        frequency: é‡é‡‡æ ·é¢‘ç‡ï¼ˆå¦‚"1h", "4h", "1d"ç­‰ï¼‰
    
    Returns:
        é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
    """
    print(f"\nğŸ”„ å°†æ•°æ®é‡é‡‡æ ·åˆ°{frequency}...")
    
    # åˆå§‹åŒ–é‡é‡‡æ ·å™¨
    resampler = HoloKlineResampler(resample_interval=frequency)
    
    resampled_files = []
    for file_path in holo_files:
        symbol = file_path.stem
        output_file = output_dir / f"{symbol}_{frequency}.parquet"
        
        try:
            # è¯»å–å…¨æ¯kçº¿æ•°æ®
            df = pl.read_parquet(file_path)
            
            # å°†DataFrameè½¬æ¢ä¸ºLazyFrame
            ldf = df.lazy()
            
            # é‡é‡‡æ ·åˆ°æŒ‡å®šé¢‘ç‡
            resampled_ldf = resampler.resample(ldf)
            
            # è®¡ç®—å¹¶ä¿å­˜é‡é‡‡æ ·åçš„æ•°æ®
            resampled_df = resampled_ldf.collect()
            resampled_df.write_parquet(output_file)
            resampled_files.append(output_file)
            
            print(f"   âœ… {symbol}: {len(df)} è¡Œ â†’ {len(resampled_df)} è¡Œ")
        except Exception as e:
            print(f"   âŒ {symbol}: é‡é‡‡æ ·å¤±è´¥ - {e}")
            import traceback
            traceback.print_exc()
    
    print(f"âœ… æˆåŠŸé‡é‡‡æ · {len(resampled_files)} ä¸ªæ–‡ä»¶åˆ°1h")
    return resampled_files


def get_final_dataframe(resampled_files: List[Path], symbol: str) -> pl.DataFrame:
    """
    è·å–æœ€ç»ˆçš„DataFrame
    
    Args:
        resampled_files: é‡é‡‡æ ·åçš„æ–‡ä»¶åˆ—è¡¨
        symbol: è¦è·å–çš„ç¬¦å·
    
    Returns:
        æœ€ç»ˆçš„DataFrame
    """
    for file_path in resampled_files:
        if file_path.stem.startswith(symbol):
            return pl.read_parquet(file_path)
    
    print(f"âŒ æ²¡æœ‰æ‰¾åˆ° {symbol} çš„é‡é‡‡æ ·æ–‡ä»¶")
    return pl.DataFrame()


async def main() -> None:
    """ä¸»å‡½æ•°"""
    # é…ç½®
    http_proxy = "http://127.0.0.1:7890"  # 7890ä»£ç†
    data_dir = Path("d:/Codes/binance_datatool-main/data")  # æ•°æ®ä¿å­˜ç›®å½•
    parsed_data_dir = Path("d:/Codes/binance_datatool-main/parsed_data")  # è§£æåçš„æ•°æ®ç›®å½•
    
    # æ—¶é—´èŒƒå›´
    start_date = "2024-01-01"
    end_date = "2024-01-02"
    
    # æµ‹è¯•å•ä¸ªè´§å¸å¯¹
    test_symbol = "BTCUSDT"
    
    try:
        # 1. è·å–æ‰€æœ‰äº¤æ˜“å¯¹
        print("ğŸ” è·å–æ‰€æœ‰UMäº¤æ˜“å¯¹...")
        all_symbols = await get_all_um_symbols(http_proxy)
        print(f"âœ… æ‰¾åˆ° {len(all_symbols)} ä¸ªUMäº¤æ˜“å¯¹")
        print(f"   ç¤ºä¾‹: {', '.join(all_symbols[:3])}...")
        
        # 2. è·å–å•ä¸ªè´§å¸å¯¹çš„Kçº¿æ•°æ®ï¼ˆé‡é‡‡æ ·åˆ°4å°æ—¶ï¼‰
        print(f"\nğŸ“Š è·å– {test_symbol} çš„Kçº¿æ•°æ®...")
        kline_df = await get_kline_dataframe(
            http_proxy=http_proxy,
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            data_dir=data_dir,
            parsed_data_dir=parsed_data_dir,
            frequency="4h"
        )
        
        if not kline_df.is_empty():
            print(f"âœ… æˆåŠŸè·å– {test_symbol} çš„Kçº¿æ•°æ®")
            print(f"   è¡Œæ•°: {len(kline_df)}")
            print(f"   åˆ—: {list(kline_df.columns)}")
            
            # ç»˜åˆ¶Kçº¿æ•°æ®çš„closeä»·æ ¼
            print(f"ğŸ“Š ç»˜åˆ¶ {test_symbol} çš„Kçº¿æ•°æ®...")
            # åˆ›å»ºoutputç›®å½•ç”¨äºä¿å­˜å›¾ç‰‡
            output_dir = Path("d:/Codes/binance_datatool-main/output")
            output_dir.mkdir(exist_ok=True)
            save_path = output_dir / f"{test_symbol}_close_{start_date}_{end_date}.png"
            plot_dataframe(kline_df, data_type='kline', symbol=test_symbol, save_path=save_path)
        
        # 3. è·å–å•ä¸ªè´§å¸å¯¹çš„Metricsæ•°æ®
        print(f"\nğŸ“Š è·å– {test_symbol} çš„Metricsæ•°æ®...")
        metrics_df = await get_metrics_dataframe(
            http_proxy=http_proxy,
            symbol=test_symbol,
            start_date=start_date,
            end_date=end_date,
            data_dir=data_dir
        )
        
        if not metrics_df.is_empty():
            print(f"âœ… æˆåŠŸè·å– {test_symbol} çš„Metricsæ•°æ®")
            print(f"   è¡Œæ•°: {len(metrics_df)}")
            print(f"   åˆ—: {list(metrics_df.columns)}")
        
        print("\nâœ… æ‰€æœ‰åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç¨‹åºå·²è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()