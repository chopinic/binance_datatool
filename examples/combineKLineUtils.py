#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½å‰2ä¸ªUMäº¤æ˜“å¸å¯¹çš„æœ€è¿‘5å¤©klineå’Œmetricsï¼ˆmeritsï¼‰æ•°æ®çš„å·¥å…·
"""

import asyncio
import os
from pathlib import Path
from datetime import datetime, timedelta
from itertools import chain
from typing import Dict, List

# å…¨å±€æ•°æ®è·¯å¾„å¸¸é‡
BASE_DATA_PATH = "data"

from bdt_common.constants import HTTP_TIMEOUT_SEC
from bdt_common.enums import DataFrequency, DataType, TradeType
from bdt_common.network import create_aiohttp_session
from bhds.aws.client import create_aws_client_from_config
from bhds.aws.downloader import AwsDownloader
from bhds.aws.checksum import ChecksumVerifier
from bhds.aws.local import AwsDataFileManager


def get_recent_days(days: int = 5) -> List[str]:
    """
    è·å–æœ€è¿‘å‡ å¤©çš„æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Args:
        days: å¤©æ•°
    
    Returns:
        æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‰ä»æ—§åˆ°æ–°æ’åº
    """
    # ä½¿ç”¨UTCæ—¶é—´ä»¥é¿å…æ—¶åŒºé—®é¢˜
    today = datetime.utcnow()
    return [(today - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(days-1, -1, -1)]


async def get_top_symbols(http_proxy: str, limit: int = 2) -> List[str]:
    """
    è·å–UMäº¤æ˜“å¯¹çš„å‰Nä¸ªå¸å¯¹
    
    Args:
        http_proxy: HTTPä»£ç†
        limit: è¿”å›çš„å¸å¯¹æ•°é‡
    
    Returns:
        å¸å¯¹åˆ—è¡¨
    """
    # ä½¿ç”¨klineæ•°æ®ç±»å‹æ¥è·å–å¸å¯¹åˆ—è¡¨
    async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
        client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.kline,
            data_freq=DataFrequency.daily,
            time_interval="1m",
            session=session,
            http_proxy=http_proxy
        )
        symbols = await client.list_symbols()
        return symbols[:limit]


async def filter_recent_files(files: List[Path], date_list: List[str]) -> List[Path]:
    """
    ç­›é€‰å‡ºæœ€è¿‘å‡ å¤©çš„æ–‡ä»¶
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        date_list: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    
    Returns:
        ç­›é€‰åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # è¿‡æ»¤æ‰CHECKSUMæ–‡ä»¶ï¼Œåªå¤„ç†zipæ–‡ä»¶
    zip_files = [f for f in files if f.name.endswith('.zip')]
    
    filtered_files = []
    
    # å…ˆè¾“å‡ºæ‰€æœ‰è·å–åˆ°çš„zipæ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"  å…±è·å–åˆ° {len(zip_files)} ä¸ªzipæ–‡ä»¶")
    if zip_files:
        print(f"  æœ€æ–°çš„5ä¸ªæ–‡ä»¶: {', '.join([f.name for f in sorted(zip_files)[-5:]])}")
    
    # æ”¹è¿›çš„æ—¥æœŸåŒ¹é…é€»è¾‘
    for file_path in zip_files:
        # æ–‡ä»¶åæ ¼å¼ï¼šSYMBOL-TIME_INTERVAL-YYYY-MM-DD.zip
        # æˆ–ï¼šSYMBOL-YYYY-MM-DD.zip
        filename = file_path.name
        
        # æå–æ–‡ä»¶åä¸­çš„æ—¥æœŸéƒ¨åˆ†
        import re
        date_match = re.search(r'\d{4}-\d{2}-\d{2}', filename)
        if date_match:
            file_date = date_match.group()
            if file_date in date_list:
                filtered_files.append(file_path)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ—¥æœŸçš„æ–‡ä»¶ï¼Œå°è¯•è·å–æœ€æ–°çš„æ–‡ä»¶
    if not filtered_files and zip_files:
        print(f"  æœªæ‰¾åˆ°åŒ¹é…æ—¥æœŸçš„æ–‡ä»¶ï¼Œå°†è·å–æœ€æ–°çš„æ–‡ä»¶")
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œé€šå¸¸æœ€æ–°çš„æ–‡ä»¶ä¼šåœ¨åé¢
        sorted_files = sorted(zip_files)
        # è¿”å›æœ€æ–°çš„æ–‡ä»¶
        filtered_files = [sorted_files[-1]]
    
    return filtered_files


async def download_data(
    http_proxy: str,
    symbols: List[str],
    data_dir: Path,
    time_interval: str = "1m",
    days: int = 5
) -> None:
    """
    ä¸‹è½½æŒ‡å®šå¸å¯¹çš„klineå’Œmetricsæ•°æ®
    
    Args:
        http_proxy: HTTPä»£ç†
        symbols: å¸å¯¹åˆ—è¡¨
        data_dir: æ•°æ®ä¿å­˜ç›®å½•
        time_interval: Kçº¿æ—¶é—´é—´éš”
        days: ä¸‹è½½å¤©æ•°
    """
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–æœ€è¿‘5å¤©çš„æ—¥æœŸ
    recent_days = get_recent_days(days)
    print(f"ğŸ“… å°†ä¸‹è½½æœ€è¿‘ {days} å¤©çš„æ•°æ®: {', '.join(recent_days)}")
    print(f"ğŸŒ å½“å‰UTCæ—¶é—´: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = AwsDownloader(local_dir=data_dir, http_proxy=http_proxy, verbose=True)
    verifier = ChecksumVerifier(delete_mismatch=False)
    
    async with create_aiohttp_session(HTTP_TIMEOUT_SEC) as session:
        # ä¸‹è½½Kçº¿æ•°æ®
        print("\nğŸ“Š å¼€å§‹ä¸‹è½½Kçº¿æ•°æ®...")
        kline_client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.kline,
            data_freq=DataFrequency.daily,
            time_interval=time_interval,
            session=session,
            http_proxy=http_proxy
        )
        
        # æ‰¹é‡è·å–æ‰€æœ‰Kçº¿æ–‡ä»¶
        kline_files_map = await kline_client.batch_list_data_files(symbols)
        
        # ç­›é€‰æœ€è¿‘5å¤©çš„Kçº¿æ–‡ä»¶
        filtered_kline_files = []
        for symbol, files in kline_files_map.items():
            recent_files = await filter_recent_files(files, recent_days)
            filtered_kline_files.extend(recent_files)
            print(f"   {symbol}: {len(recent_files)}ä¸ªKçº¿æ–‡ä»¶")
        
        if filtered_kline_files:
            print(f"ğŸ“¥ æ€»å…±ä¸‹è½½ {len(filtered_kline_files)} ä¸ªKçº¿æ–‡ä»¶")
            await downloader.aws_download(filtered_kline_files)
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„Kçº¿æ–‡ä»¶")
        
        # ä¸‹è½½Metricsæ•°æ®ï¼ˆä½œä¸ºmeritsçš„æ›¿ä»£ï¼‰
        print("\nğŸ“Š å¼€å§‹ä¸‹è½½Metricsæ•°æ®...")
        metrics_client = create_aws_client_from_config(
            trade_type=TradeType.um_futures,
            data_type=DataType.metrics,
            data_freq=DataFrequency.daily,
            time_interval=None,  # metricsä¸éœ€è¦time_interval
            session=session,
            http_proxy=http_proxy
        )
        
        # æ‰¹é‡è·å–æ‰€æœ‰Metricsæ–‡ä»¶
        metrics_files_map = await metrics_client.batch_list_data_files(symbols)
        
        # ç­›é€‰æœ€è¿‘5å¤©çš„Metricsæ–‡ä»¶
        filtered_metrics_files = []
        for symbol, files in metrics_files_map.items():
            recent_files = await filter_recent_files(files, recent_days)
            filtered_metrics_files.extend(recent_files)
            print(f"   {symbol}: {len(recent_files)}ä¸ªMetricsæ–‡ä»¶")
        
        if filtered_metrics_files:
            print(f"ğŸ“¥ æ€»å…±ä¸‹è½½ {len(filtered_metrics_files)} ä¸ªMetricsæ–‡ä»¶")
            await downloader.aws_download(filtered_metrics_files)
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„Metricsæ–‡ä»¶")
    
    # éªŒè¯æ–‡ä»¶
    print("\nğŸ” å¼€å§‹éªŒè¯æ–‡ä»¶...")
    all_unverified_files = []
    for symbol in symbols:
        # éªŒè¯Kçº¿æ–‡ä»¶
        kline_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.kline.value}/{symbol}/{time_interval}"
        if kline_symbol_dir.exists():
            manager = AwsDataFileManager(kline_symbol_dir)
            all_unverified_files.extend(manager.get_unverified_files())
        
        # éªŒè¯Metricsæ–‡ä»¶
        metrics_symbol_dir = data_dir / f"{BASE_DATA_PATH}/{TradeType.um_futures.value}/daily/{DataType.metrics.value}/{symbol}"
        if metrics_symbol_dir.exists():
            manager = AwsDataFileManager(metrics_symbol_dir)
            all_unverified_files.extend(manager.get_unverified_files())
    
    if all_unverified_files:
        results = verifier.verify_files(all_unverified_files)
        print(f"âœ… éªŒè¯å®Œæˆ: {results['success']} ä¸ªæˆåŠŸ, {results['failed']} ä¸ªå¤±è´¥")
    else:
        print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²éªŒè¯è¿‡")
    
    print("\nğŸ‰ æ•°æ®ä¸‹è½½å®Œæˆï¼")


async def test_download() -> None:
    """æµ‹è¯•ä¸‹è½½åŠŸèƒ½ï¼Œåªä¸‹è½½å°‘é‡æ–‡ä»¶"""
    # é…ç½®
    http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy") or "http://127.0.0.1:7890"
    data_dir = Path("d:/Codes/binance_datatool-main/data")  # æ•°æ®ä¿å­˜ç›®å½•
    time_interval = "1m"  # Kçº¿æ—¶é—´é—´éš”
    days = 1  # æµ‹è¯•æ—¶åªä¸‹è½½1å¤©çš„æ•°æ®
    
    print("ğŸš€ å¯åŠ¨Binance UMäº¤æ˜“å¯¹æ•°æ®ä¸‹è½½å·¥å…·")
    print(f"ğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {data_dir}")
    print(f"ğŸŒ HTTPä»£ç†: {http_proxy if http_proxy else 'æ— '}")
    
    # è·å–å‰2ä¸ªUMäº¤æ˜“å¯¹
    print("\nğŸ” è·å–äº¤æ˜“å¯¹åˆ—è¡¨...")
    try:
        symbols = await get_top_symbols(http_proxy, limit=4)
        symbols = ["BTCUSDT", "ETHUSDT"]

        print(f"ğŸ“‹ é€‰æ‹©çš„äº¤æ˜“å¯¹: {', '.join(symbols)}")
    except Exception as e:
        print(f"âŒ è·å–äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥: {e}")
        return
    
    # ä¸‹è½½æ•°æ®
    try:
        await download_data(http_proxy, symbols, data_dir, time_interval, days)
    except Exception as e:
        print(f"âŒ ä¸‹è½½æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main() -> None:
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(test_download())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ä¸‹è½½å·²è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()